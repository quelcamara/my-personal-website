{"props":{"pages":[{"__metadata":{"id":"content\\pages\\about.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"about.md","relProjectPath":"content\\pages\\about.md","modelType":"page","modelName":"page","modelLabel":"Page","urlPath":"/about"},"frontmatter":{"title":"About Me","subtitle":"I am so glad to have you here!","img_path":"images/about.jpg","seo":{"title":"About Me","description":"A page about me and my work","extra":[{"name":"og:type","value":"website","keyName":"property"},{"name":"og:title","value":"About Me","keyName":"property"},{"name":"og:description","value":"A page about me and my work","keyName":"property"},{"name":"og:image","value":"images/about.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"About Me"},{"name":"twitter:description","value":"A page about me and my work"},{"name":"twitter:image","value":"images/about.jpg","relativeUrl":true}]},"layout":"page"},"markdown":"\nWelcome to the most beginner-friendly blog on **Data Science** you will ever see! Hope you are ready to go into the hotter topics on **Data Analysis**, **Machine Learning**, **Artificial Inteligence**, and so many other exciting stuff about technology and programming without getting all chaotic about those hard and complex book definitions.\n\nSo let us not take it too long!</br>\nBut first, let me introduce myself properly.\n\nI am a former Designer and Civil Engineer who ended up falling in love with Data Science after a long time struggling to decide if I should keep on following the path I had once begun, or if I should just throw it all away and follow my heart into something I could finally see myself in. I know it might sound a little cheesy at some point, but that's what it is.  \n\nI am a very curious person, and when I first dumped into Data Science, it was such a completely new world for me that I found myself unbearable amazed by the so many things I felt I just *needed* to know right away.\n\n>Any sufficiently advanced technology is indistinguishable from magic. <cite>Arthur C. Clarke</cite>\n\n So here we are!\n\nWhen I decided I wanted to keep track of my progress I found that building a blog would be just the perfect beginning. That's because here I can not only keep my records about the things I am most recently studying, but I can also improve some communication skills, and - last but not least! - maybe I am able to help other people, in the near future, who are also starting their jorney into Data Science.\n\nNo changing of career is easy as it seems. It demands a lot of effort and resilience from you to start all over again. And that's why I am here! I hope this blog finds you in a perfect timing so you don't give up on following the path you've been dreaming.\n\n*Be very welcome and enjoy your stay!* 💻\n"},{"__metadata":{"id":"content\\pages\\contact.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"contact.md","relProjectPath":"content\\pages\\contact.md","modelType":"page","modelName":"contact","modelLabel":"Contact","urlPath":"/contact"},"frontmatter":{"title":"Get in Touch","img_path":"images/contact.jpg","form_id":"contactForm","form_action":"/success","form_fields":[{"input_type":"text","name":"name","label":"Name","default_value":"Your name","is_required":true},{"input_type":"email","name":"email","label":"Email","default_value":"Your email address","is_required":true},{"input_type":"select","name":"subject","label":"Subject","default_value":"Please select","options":["Get in touch","Post suggestions","Partnership","Error on the site","Other"]},{"input_type":"textarea","name":"message","label":"Message","default_value":"Your message"},{"input_type":"checkbox","name":"consent","label":"I understand that this form is storing my submitted information so I can be contacted."}],"submit_label":"Send Message","seo":{"title":"Get in Touch","description":"This is the contact page","extra":[{"name":"og:type","value":"website","keyName":"property"},{"name":"og:title","value":"Get in Touch","keyName":"property"},{"name":"og:description","value":"This is the contact page","keyName":"property"},{"name":"og:image","value":"images/contact.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"Get in Touch"},{"name":"twitter:description","value":"This is the contact page"},{"name":"twitter:image","value":"images/contact.jpg","relativeUrl":true}]},"layout":"contact"},"markdown":"\nTo get in touch fill the form below.\n"},{"__metadata":{"id":"content\\pages\\index.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"index.md","relProjectPath":"content\\pages\\index.md","modelType":"page","modelName":"home","modelLabel":"Home","urlPath":"/"},"frontmatter":{"title":"Home","has_more_link":true,"more_link_text":"Keep reading","seo":{"title":"Raquel Câmara - Data Scientist","description":"A Data Scientist Diary","extra":[{"name":"og:type","value":"website","keyName":"property"},{"name":"og:title","value":"Stackbit Fjord Theme","keyName":"property"},{"name":"og:description","value":"A Data Scientist Diary","keyName":"property"},{"name":"og:image","value":"images/home.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"Stackbit Fjord Theme"},{"name":"twitter:description","value":"A Data Scientist Diary"},{"name":"twitter:image","value":"images/home.jpg","relativeUrl":true}]},"layout":"home"},"markdown":""},{"__metadata":{"id":"content\\pages\\portfolio.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"portfolio.md","relProjectPath":"content\\pages\\portfolio.md","modelType":"page","modelName":"portfolio","modelLabel":"Portfolio","urlPath":"/portfolio"},"frontmatter":{"title":"Portfolio","subtitle":"Topic under construction.","img_path":"images/portfolio.jpg","seo":{"title":"Portfolio","description":"My data science portfolio","extra":[{"name":"og:type","value":"website","keyName":"property"},{"name":"og:title","value":"Portfolio","keyName":"property"},{"name":"og:description","value":"My data science portfolio","keyName":"property"},{"name":"og:image","value":"images/portfolio.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"Portfolio"},{"name":"twitter:description","value":"My data science portfolio"},{"name":"twitter:image","value":"images/portfolio.jpg","relativeUrl":true}]},"layout":"portfolio"},"markdown":""},{"__metadata":{"id":"content\\pages\\portfolio\\insight-project-1.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"portfolio\\insight-project-1.md","relProjectPath":"content\\pages\\portfolio\\insight-project-1.md","modelType":"page","modelName":"post","modelLabel":"Post","urlPath":"/portfolio/insight-project-1"},"frontmatter":{"title":"Insight Project","subtitle":"Understanding the businesses problems is a key role to perform a good analysis. It's not unusual for the companies to not know exactly what their problems roots are. And that's when a data scientist will make a huge difference.","excerpt":"Explanatory Data Analysis to Build some Buy&Sell House Company Insights. The goal was to provide the company useful insights so they can better understand how the market and also their own businesses behave.","date":"2021-04-06","thumb_img_path":"images/portfolio-1/01.jpg","thumb_img_alt":"Greener Houses","content_img_path":"images/portfolio-1/01.jpg","seo":{"title":"Insight Project","description":"Understanding the businesses problems is a key role to perform a good analysis. It's not unusual for the companies to not know exactly what their problems roots are. And that's when a data scientist will make a huge difference.","extra":[{"name":"og:type","value":"article","keyName":"property"},{"name":"og:title","value":"Insight Project","keyName":"property"},{"name":"og:description","value":"Understanding the businesses problems is a key role to perform a good analysis. It's not unusual for the companies to not know exactly what their problems roots are. And that's when a data scientist will make a huge difference.","keyName":"property"},{"name":"og:image","value":"images/portfolio-1/01.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"Insight Project"},{"name":"twitter:description","value":"Understanding the businesses problems is a key role to perform a good analysis. It's not unusual for the companies to not know exactly what their problems roots are. And that's when a data scientist will make a huge difference."},{"name":"twitter:image","value":"images/portfolio-1/01.jpg","relativeUrl":true}]},"layout":"post"},"markdown":"\n<p align=\"left\"><a href=\"https://github.com/quelcamara/startup-success-analysis\"><img src=\"https://img.shields.io/badge/Language-Python-pink\"></a> <a href=\"https://github.com/quelcamara/startup-success-analysis\"><img src=\"https://img.shields.io/badge/Project%20Status-Finished-lightblue\"></a></p>\n\n## Table of Contents\n* [About the project](#about-the-project)\n* [Overall description](#overall-description)\n* [Becoming familiar](#becoming-familiar)\n  * [Business problem](#business-problem)\n  * [Solution planning](#solution-planning)\n  * [3 relevant insights](#3-relevant-insights)\n  * [Financial results](#financial-results)\n  * [Conclusion](#conclusion)\n* [Technologies](#technologies)\n* [Setups for Project Execution](#setups-for-project-execution)\n  * [Requirements](#requirements)\n  * [Installation](#installation)\n  * [Executing online](#executing-online)\n  * [Executing via cmd](#executing-via-cmd)\n* [Author](#author)\n\n### About the project\nThis is an insight project developed to be part of my data scientist portfolio. It is basically shaped as an explanatory data analysis to help a fictional company's decision makers to know whether to invest or not in a particular house.\n\nThe company, named Greener Houses, is in the market for 8 yrs with a business model focused on buying and selling houses through its digital platform. It is a very technological company and it performs all services based on market analysis decision making.\n\nThe main goal was to provide the company useful insights so they can better understand how the market behaves and also their own businesses. This way, they can see the hole picture for the businesses to come.\n\nAccess the complete project code [here](../../../images/portfolio-1/Greener_Houses.html) in html version. Alternatively, if you rather download the jupyter notebook, go to the GitHub repository [here](https://github.com/quelcamara/ds-portfolio-insight-project-01).\n\n### Overall description\nThe project was divided into 6 sections, each one of them also divided into subtopics. This repository includes the .csv file and a directory where the plots were saved. You will see under the sections:\n\n0. Business and Analysis Understanding:\n   - **Business Problem**: states the business questions to be answered along the analysis.\n   - **Business Assumptions**: stated the assumptions we make before starting manipulating the dataset. It includes the method we should use in case we need to remove outliers.\n   - **Dataset Description**: describes columns and data types for each attribute on the dataset.\n   - **Solution Planning**: describes the steps to be taking along the data analysis in order to achieve a good business problem resolution.  \n   - **Hypothesis**: states the initial assumptions we made before starting manipulating the dataset. These hypothesis guide us throughout the analysis.\n\n\n1. Imports:\n   - Loads packages, functions and the .csv file into the Jupyter Lab environment. \n\n\n2. Descriptive Analysis:\n   - It is the stage at which we become familiar with the data. This section includes measures of some statistical variables, such as *mean*, *median*, *mode*, *range*, and so on. It also includes data types, missing and duplicated values checking. Below is an example table for this section:\n\n![image](https://user-images.githubusercontent.com/73648823/114231149-6d975600-9950-11eb-98c8-7da8496c01f9.png)\n\n3. Data Cleaning and Preparation:\n   - Under this section, we discuss the need for preparing the dataset by cleaning unwanted data or not, depending on our goal.\n   - It also includes a *correlation matrix* plot to state how the attributes relate with one another.\n\n\n4. Exploratory Data Analysis:\n   - **Business Problem Nº1**: contains the resolution for the first business problem.\n   - **Business Problem Nº2**: contains the resolution for the second business problem.\n   \n\n5. Communicating Results:\n   - **Evaluating Hypothesis**: determines whether to support or reject the hypothesis.\n   - **Graphical Data Visualization**: illustrates some of the results graphically.\n  \n\n6. Conclusion\n   - Summarize the results by given the company a *prospective financial return* based on the analysis' suggestions. \n\n### Becoming familiar\n\n#### Business problem\nThe questions answered on this analysis are:\n- Which houses should Greener Houses buy and how much should the company pay for them?\n- Once the house is acquired, when should it be sold and at what price?\n- Should Greener Houses renovate a house to increase the sale price?\n\n#### Solution planning\nThe main steps taking to solve the business problem were as summarized below:\n- Collecting and organizing data;\n- Grouping data per region (zipcode);\n- Finding the houses prices median for each region;\n- Suggesting to buy the houses in good conditions that have prices lower than the median of its region.\n- Grouping data per seasonality;\n- Finding the new houses prices median within each set of region and seasonality;\n- Updating the suggestions based on the set of conditions analyzed.\n\n#### 3 relevant insights\nAfter carefully studying the dataset, some insights could be revealed. Three of them are listed below:\n\n1. When analyzing *if houses overlooking the waterfront were about 30% more expensive than houses not overlooking the waterfront on average*, we found out that, actually, their prices are more than **200%** higher comparing.\n\n![image](https://github.com/quelcamara/ds-portfolio-insight-project-01/blob/main/plots/average-prices-for-houses-either-overlooking-or-not-the-waterfront.png?raw=true)\n\n2. While expecting that houses with basements would have a total area (square foot lot) 40% larger than houses without basements on average, we found out that, on the opposite, houses without basements have a total area larger than houses with this spare room. This difference was calculated on 18% on average.\n\n![image](https://user-images.githubusercontent.com/73648823/114230229-40967380-994f-11eb-8796-67fae7e46832.png)\n\n3. If we first thought that houses with 3 bathrooms could have a MoM positive growth within time, on the analysis we actually discovered that they don't seem to have a meaningful growth and that their prices variation were very *miscellaneous*.\n\n![image](https://user-images.githubusercontent.com/73648823/114230160-28265900-994f-11eb-8433-68cfe5cc4a87.png)\n\n\n#### Financial results\nAs a result of this analysis, the company Greener Houses has about 11840 good deals on the construction market.\n\nIf the company chooses to invest only on the two more profitable regions out of the seventy available, it would still have 216 houses stated as good deals. This would represent a rate of 3 to 4 houses per month over the next 4 to 6 years, totalizing an average profit of U$234.344,38 per house and about U$700.000 to U$1.000.000 per month.\n\n#### Conclusion\nWith this analysis, Greener Houses is now capable of making its decisions based on the market opportunities, the financial returns, and also on the company revenue available to invest on these businesses. Moreover, the best time for selling and the profits for each deal will also be available for the company to evaluate prior to the decision-making.\n\nIt is then possible to say that the main goal of this analysis could be achieved.\n\n### Technologies\nThis project was built with the following technologies:\n* [Python](https://www.python.org/downloads/) --version: 3.9.2\n* [Jupyter Lab](https://jupyter.org/install) --version: 3.0.10\n* [Pandas](https://pandas.pydata.org/) --version: 1.2.3\n* [Numpy](https://numpy.org/) --version: 1.19.2\n* [Matplotlib](https://matplotlib.org/) --version: 3.2.1\n* [Seaborn](https://seaborn.pydata.org/) --version: 0.11.1\n\n### Setups for Project Execution\nIf you find it better to run the project on your computer rather than access the .html content, here are two options for you to open the .ipynb file: on your own localhost, or online.\n\nIf your only need is to see the notebook, there is no installation required, just jump into the [Executing online](#executing-online) topic and follow the directions.\n\nHowever, if you want to edit or manipulate the code, it will be necessary to have some basic configurations done on your machine to execute the project. See the on the topic [Requirements](#requirements) below.\n\n:bulb: You also won't be able to manipulate the file if you don't have your local server running the `Jupyter Lab`. For this, you can have a full explanation on the [Executing via cmd](#game_die-executing-via-cmd) step.\n\n#### Requirements\nBefore you get started, you will need to have [Python](https://www.python.org/downloads/) installed on your machine as well as the [Jupyter Notebook or Jupyter Lab](https://jupyter.org/install). There is no need to have any specific code editor or IDE if you wish to work on the code - you can do it directly on the notebooks.\n\nSince it is not externally deployed, you will need to download the [ds-portfolio-insight-project-01](https://github.com/quelcamara/ds-portfolio-insight-project-01) project into your computer to be able to run it locally.\n\n❗ But remember, if you only want to snoop into the code and have no interest on changing it, there is no need to trouble with this. You can also forget about the [Installation](#wrench-installation) and just go directly to the topic [Executing online](#globe_with_meridians-executing-online).\n\n##### Installation\nIf you have different versions of the technologies used to create this project, no need to panic! It won't be necessary to uninstall everything. You can always give it a try on running it on you machine with the current versions you already have and see if it works. If something goes wrong - because packages and functions are constantly being updated - you can easily create a virtual environment and then install only what you need.\n\nYou can follow the steps below if you need to create and configure a virtual environment. These are for Windows command-line, if you are a Linux or MacOS user, you will need to look for the likely commands. On the Command Prompt (cmd):\n\n```shell\n# Open the project directory (full path here)\n$ cd C:\\..\\portfolio-insight-project\n\n# Create the virtual environment (choosing python 3.9.2)\n$ virtualenv venv --python=python3.9.2\n\n# Access the virtual environment\n$ venv\\Scripts\\activate.bat\n```\nAfter that, you will see a `(venv)` sign on the command-line indicating that you are now inside the virtual environment. It should look like this:\n```shell\n$ (venv) C:\\..\\portfolio-insight-project>_\n```\nFinally, you can install the technologies you need with whatever version you want inside your environment, and it won't affect at all any package you have on your operational system. Use the `pip install` for that:\n\n> Just a friendly reminders here!<br/>\nIf you have installed <b>Anaconda</b> on your computer at any moment in your life, you probably already have all those packages available for you. Check for their versions and, if you have any troubles executing the Notebooks, create the virtual environment and install the packages using `conda` instead of `pip`.\n\n```shell\n$ pip install pandas==1.2.3\n$ pip install numpy==1.19.2\n$ pip install matplotlib==3.2.1\n$ pip install seaborn==0.11.1\n```\nWith these configurations, your machine is now ready to run and manipulate the project.\n\n##### Executing online\nTo run the notebook online, you'll first need to download the [Greener_Houses.ipynb](https://github.com/quelcamara/ds-portfolio-insight-project-01/blob/main/Greener_Houses.ipynb) file on this repository.\n\nThen, you can access a Jupyter Lab free and online server [here](https://jupyter.org/try). Scroll over the page and choose the JupyterLab option. It will redirect you to an online version of the tool that will allow you to open the file you have just downloaded.\n\nGo to your local repository where the downloaded file is and then drag the .ipynb file into the left sidebar of the online tool. The field should be seen with a hashed border before you drop it. Wait a little bit while it is loaded and then just double click on the project once it is there under the sidebar options.\n\n##### Executing via cmd\nTo run the project and have access to the Notebooks:\n```shell\n# Go to the project directory (full path here)\n$ cd C:\\..\\portfolio-insight-project\n\n# Open the directory on the Jupyter Lab\n$ jupyter lab\n```\nJust be patient here if your computer takes a little while without seeming to do a thing at all. It will process your inquisition and then open an external page on the web (your default browser) running the server on your localhost. You don't need to enter anything else on the command-line until this page is opened.\n"},{"__metadata":{"id":"content\\pages\\posts\\3-mighty-libraries-for-a-data-science-beginner-to-become-friends-with-part-1.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"posts\\3-mighty-libraries-for-a-data-science-beginner-to-become-friends-with-part-1.md","relProjectPath":"content\\pages\\posts\\3-mighty-libraries-for-a-data-science-beginner-to-become-friends-with-part-1.md","modelType":"page","modelName":"post","modelLabel":"Post","urlPath":"/posts/3-mighty-libraries-for-a-data-science-beginner-to-become-friends-with-part-1"},"frontmatter":{"title":"3 mighty libraries for a Data Science beginner to become friends with (part 1)","subtitle":"I will tell you the secret for this friendship to work, and why you should stick with them forever.","excerpt":"It is very common to get confused about what we should learn at the very beginning of our studies. But no need to start yelling at a cat! I am sure that after these posts you will be gifted with plenty of working for the next few days. On this first section out of three posts, we'll discover the strength of Pandas.","date":"2021-05-13","thumb_img_path":"images/post-2/cover.jpg","thumb_img_alt":"migthy libraries","content_img_path":"images/post-2/cover.jpg","seo":{"title":"3 mighty libraries for a Data Science beginner to become friends with (part 1)","description":"I will tell you the secret for this friendship to work, and why you should stick with them forever.","extra":[{"name":"og:type","value":"article","keyName":"property"},{"name":"og:title","value":"3 mighty libraries for a Data Science beginner to become friends with (part 1)","keyName":"property"},{"name":"og:description","value":"I will tell you the secret for this friendship to work, and why you should stick with them forever.","keyName":"property"},{"name":"og:image","value":"images/post-2/cover.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"3 mighty libraries for a Data Science beginner to become friends with (part 1)"},{"name":"twitter:description","value":"I will tell you the secret for this friendship to work, and why you should stick with them forever."},{"name":"twitter:image","value":"images/post-2/cover.jpg","relativeUrl":true}]},"layout":"post"},"markdown":"\r\nI understand you.\r\n\r\nWhen we start something new, we use to get so excited that we begin to google search a lot of random stuff about that thing. And it becomes almost inevitable not to get lost on the endless sea of information available on the internet. But by putting some effort, we are able to filter what is important to know based on our current level.\r\n\r\nOn this set of posts, you will be given precious information about three **python** libraries that you should definitely look up:\r\n\r\n- Pandas\r\n- NumPy\r\n- Matplotlib\r\n\r\nYou probably already heard a thing or two about any of these. But if you didn't pay attention before - because maybe you thought they were not worth it - this is your chance to redeem yourself.\r\n\r\nTo better illustrate the next topics, consider the following dataset for demonstration purposes:\r\n\r\n&nbsp; | PurchaseDate |   Region  |   State   | Seller |  Item   | Units | UnitPrice  \r\n:-----:| :-----------:| :--------:| :--------:| :-----:| :------:| :----:| :-------:\r\n   0   | 10-Jun-2020  | Northeast |   Bahia   | Tobias | Stove   |  62   |  400.99\r\n   1   | 11-Jun-2020  | Southeast | São Paulo | Nadia  | Fridge  |  29   |  100.99\r\n   2   |  3-Aug-2020  | Northeast |   Ceará   | Carlos | Stove   |  55   | 1200.49\r\n   3   | 22-Aug-2020  | Northeast |   Bahia   | Pedro  | Fridge  |  81   | 1900.99\r\n   4   | 26-Aug-2020  |  Midwest  |   Goiás   | Tania  | Blender |  42   | 2300.95\r\n   5   | 10-Sep-2020  | Northeast |  Sergipe  | Tobias | Carpet  |  35   |  400.99\r\n   6   | 12-Sep-2020  |   North   |   Pará    | Carlos | Carpet  |   3   | 2750.00\r\n   7   |  7-Oct-2020  | Northeast |  Sergipe  | Nadia  | Blender |   2   | 1250.00\r\n   8   | 15-Oct-2020  |   North   | Amazonas  | Pedro  | Stove   |   7   | 1000.29\r\n   9   | 27-Nov-2020  | Southeast | São Paulo | Nadia  | Fridge  |  16   | 1500.99\r\n  10   | 13-Dec-2020  |   South   |  Paraná   | Tania  | Blender |  76   | 1450.99\r\n\r\n***file:*** order_data.csv\r\n\r\nOn this first post, we'll start with **Pandas**. And if you want to check on some specific functionality, I will leave you the table below so you can also go straight to the point.\r\n\r\n|    Load and Transform     |     Visualize     |           Locate           |       Summarize       |  \r\n:-------------------------: | :---------------: | :------------------------: | :-------------------: | \r\n[read_csv](#read-csv)       | [head](#head)     | [loc](#loc)                | [describe](#describe) |\r\n[read_excel](#read-excel)   | [tail](#tail)     | [iloc](#iloc)              | [info](#info)         |\r\n[sort_values](#sort-vals)   | [shape](#shape)   | [duplicated](#duplic)      | [sum](#sum)           |\r\n[set_index](#set-index)     | [index](#index)   | [query](#query)            | [count](#count)       |\r\n[reset_index](#reset-idx)   | [columns](#cols)  | [df['col']](#df-col1)      | [min](#min)           |\r\n[drop](#drop)               | [dtypes](#dtypes) | [df.your_col](#df-col2)    | [max](#max)           |\r\n[copy](#copy)               | [isnull](#isnull) |          ---               | [mean](#mean)         |\r\n---                         | [values](#values) |          ---               | [median](#median)     |\r\n---                         |        ---        |          ---               | [corr](#corr)         |\r\n\r\n\r\n## Pandas\r\n![Panda](../../images/post-2/01-panda.jpg)\r\n***image*** by [Eric Baccega](https://onebigphoto.com/giant-panda-sleeping-on-a-tree-china/)\r\n\r\nPandas is a very powerful python library widely used by data scientists and/or analysts for both manipulating and analysing data. It also works well with many other python modules, and its main advantage is having  an intuitive and practical usability without compromising its functionality.\r\n\r\nFor convenience, pandas use to be loaded into the project with the allias `pd`, as shown below:\r\n\r\n~~~python\r\nimport pandas as pd\r\n\r\n~~~\r\n\r\nIt is useful so we can just use this short allias instead of typing the whole package name every time we want to use a pandas function. This library also allows us to create two type of structures that make the manipulating easier: **Series** and **Dataframes**.\r\n\r\n#### Series\r\nAccording to pandas official [documentation](https://pandas.pydata.org/docs/index.html), a series is a one-dimensional ndarray (an array that belongs to the NumPy class `ndarray`) with axis labels.\r\n\r\n*Omg, this...*\r\n\r\nBetter saying, a pandas series is nothing but an unidimensional array (having a unique dimension) which can store any sort of data with labels or indexes as the axis. In short, it is like a column of a dataframe. Let's see how to create a series in pandas and how to identify its structure. The following code can be replicated in your jupyter notebook:\r\n\r\n~~~python\r\n# imports\r\nIn [1]: import pandas as pd\r\n\r\n# creates a series to store people names\r\nIn [2]: names = pd.Series(['Carlos', 'Sara', 'Louise', 'James'])\r\n\r\nIn [3]: names\r\nOut[3]: 0   Carlos\r\n        1     Sara\r\n        2   Louise\r\n        3    James\r\n        dtype: object\r\n\r\n~~~\r\n\r\nNote that, as indexation in python begins with 0, the index range of the series created goes from 0 to `n-1`, where *n* is the number of elements on your series. This is a very important thing to remember, and which might cause a lot of malfunctioning on your codes if you mess that up.\r\n\r\nWe can also see that, as I haven't specified any index range for my series, pandas automaticatly insert the standard indexation. However, if I want a different label for my axis, I can specify that when creating the object. See how it looks:\r\n\r\n~~~python\r\n# creates a series to store animals species\r\nIn [2]: animals = pd.Series(['Dog', 'Elephant', 'Fox', 'Eagle'],\r\n                            index=['A', 'B', 'C', 'D'])\r\n\r\nIn [3]: animals\r\nOut[3]: A        Dog\r\n        B   Elephant\r\n        C        Fox\r\n        D      Eagle\r\n        dtype: object\r\n\r\n~~~\r\n\r\nThese are, in fact, very simple examples. But they might help you having an ideia of what a series look like.\r\n\r\n#### Dataframes\r\nDifferently from a series, a pandas dataframe is a two-dimensional tabular structure where data is labeled by its own combination of column and row. This structure is size-mutable and potentially heterogeneous. That is, we can easily create a dataframe with two columns and two rows, and then add new columns and rows for this same object. And we can store different types of data on the same dataframe, which can be very convinient in many situations.\r\n\r\nLet's see how it works on the jupyter notebook:\r\n\r\n~~~python\r\n# imports\r\nIn [1]: import pandas as pd\r\n\r\n# creates a dataframe to store people names, ages, and heights\r\nIn [2]: names = pd.DataFrame([['Carlos', 27, 1.78], ['Sara', 12, 1.35],\r\n                              ['Louise', 35, 1.62], ['James', 18, 1.87]],\r\n                              columns=['name', 'age', 'height'],\r\n                              index=['i', 'ii','iii', 'iv'])\r\n\r\nIn [3]: names\r\nOut[3]:      name   age   height\r\n        i  Carlos    27     1.78\r\n       ii    Sara    12     1.35\r\n      iii  Louise    35     1.62\r\n       iv   James    18     1.87\r\n~~~\r\n\r\nCould you notice how they differ? Now we have a table storing a set of information that we can either access by the index - returning all the information in a the row - or by the combination of index and columns - returning a single desired value.\r\n\r\nAnd if we just want to go on regular indexing, we only need to remove the `index` attribute from the function.\r\n\r\nSee the official [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) for more information.\r\n\r\nJust so to remind you, all the operations from now on will take into consideration the dataset we defiined on the very beginning of this post. Also, see that you will face many abbreviations and acronyms on your way through the data science world. I will provide you a cheatsheet on later posts. \r\n\r\nNow, let us finally see what pandas can bring us!\r\n\r\n#### Loading data\r\nPandas has many functions to load data into your project. You can mine data from a csv - a text file - or an excel spreadsheet, for instance. But it is also possible to get information from SQL and HTML tables, SQL query, JSON strings, Google BigQuery, Stata .dta files, and so on. See [here](https://pandas.pydata.org/docs/reference/io.html) all the options pandas offers.\r\n\r\nHere are two frequently used functions:\r\n\r\n<a id=\"read-csv\"></a>\r\n###### pd.read_csv\r\nFor comma-separated text files.\r\n\r\n~~~python\r\n# locate and indicate your file path\r\ndf = pd.read_csv('C:\\...\\order_data.csv')\r\n~~~\r\n\r\n<a id=\"read-excel\"></a>\r\n###### pd.read_excel\r\nFor excel files.\r\n\r\n~~~python\r\n# locate and indicate your file path\r\n# you can also indicate the sheet name if there are more than one\r\ndf = pd.read_csv('C:\\...\\your_file_here.xlsx',\r\n                      sheet_name='sheet_name_here')\r\n~~~\r\n\r\n#### Transforming data\r\nSometimes we only need to perform some basic transformations on our DataFrame and that's where this functions come in handy.\r\n\r\n<a id=\"sort-vals\"></a>\r\n###### df.sort_values\r\nSort DataFrame by the values of chosen column or columns. It is possible to sort twice if you pass more than one column as parameter. You can also choose the direction of your sorting by assigning `ascending` as `True` or `False`.\r\n\r\nSee the [documentation](#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html).\r\n\r\n~~~python\r\n# sorts in ascending order by the column 'Seller'\r\ndf.sort_values(by='Seller', ascending=True)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/01.png)\r\n\r\n<a id=\"set-index\"></a>\r\n###### df.set_index\r\nSet an existing DataFrame column as the index. You can either use it to replace the original index or to expand it.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html).\r\n\r\n~~~python\r\n# using append=True to expand the index\r\n# column 'Seller' is picked\r\ndf.set_index('Seller', append=True)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/02.png)\r\n\r\n<a id=\"reset-idx\"></a>\r\n###### df.reset_index\r\nReset the index of a DataFrame, using the default one instead. Default indexation in python begin with 0. You can either drop the current index or insert it as a column into the DataFrame. \r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html)\r\n\r\n~~~python\r\n# drop=False keeps the current index as a column\r\ndf.reset_index(drop=False)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/03.png)\r\n\r\n<a id=\"drop\"></a>\r\n###### df.drop\r\nRemove rows or columns by specifying row index or column name to drop.<br/>\r\nFor `axis=0`, the function searchs through the DataFrame indexes. <br/>\r\nFor `axis=1`, it searchs through its columns.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html)\r\n\r\n~~~python\r\n# removes PurchaseDate from the columns\r\ndf.drop('PurchaseDate', axis=1)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/04.png)\r\n\r\n<a id=\"copy\"></a>\r\n###### df.copy\r\nMake a copy of the object's indices and data. If you set `deep=True`, none of the modifications on the original object will reflect on the copy. However, setting `deep=False` makes a shadow copy, and any modification on either the original or the shadow object will reflect on each other.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html)\r\n\r\n~~~python\r\n# creates a copy df\r\n# deep=True is the default parameter\r\ndf_2 = df.copy()\r\nprint(df_2)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/05.png)\r\n\r\n#### Visualizing data\r\n\r\n<a id=\"head\"></a>\r\n###### df.head\r\nReturn the **first** *n* rows. If `n` is not specified, it returns the first 5 rows as default. It is aso possible to return everything except the last *n* rows by passing a negative parameter for `n`.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)\r\n\r\n~~~python\r\n# returns the first 3 rows\r\ndf.head(3)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/06.png)\r\n\r\n<a id=\"tail\"></a>\r\n###### df.tail\r\nReturn the **last** *n* rows. If `n` is not specified, it returns the last 5 rows as default. It is aso possible to return everything except the first *n* rows by passing a negative parameter for `n`.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tail.html)\r\n\r\n~~~python\r\n# returns everything except the first 6 rows\r\ndf.tail(-6)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/07.png)\r\n\r\n<a id=\"shape\"></a>\r\n###### df.shape\r\nReturn a tuple representing the dimensionality of the DataFrame. The first element represents the total number of rows, and the second element is the total number of columns.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html)\r\n\r\n~~~python\r\n# returns the DataFrame dimensionality\r\ndf.shape\r\n~~~\r\n\r\n![Image](../../../../images/post-2/08.png)\r\n\r\n<a id=\"index\"></a>\r\n###### df.index\r\nReturn the index (row labels) of the DataFrame. If the object index is the default indexation, it will return a RangeIndex object with `start`, `stop`, and `step` parameters.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.index.html)\r\n\r\n~~~python\r\n# returns rows labels\r\ndf.index\r\n~~~\r\n\r\n![Image](../../../../images/post-2/09.png)\r\n\r\n<a id=\"cols\"></a>\r\n###### df.columns\r\nReturn a list of the columns labels of the DataFrame.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html)\r\n\r\n~~~python\r\n# returns columns labels\r\ndf.columns\r\n~~~\r\n\r\n![Image](../../../../images/post-2/10.png)\r\n\r\n<a id=\"dtypes\"></a>\r\n###### df.dtypes\r\nIt returns a Series with the data type of each column. If there are columns with mixed types, they'll be stored with the *object* `dtype`.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html)\r\n\r\n~~~python\r\n# returns the data type of each column\r\ndf.dtypes\r\n~~~\r\n\r\n![Image](../../../../images/post-2/11.png)\r\n\r\n<a id=\"isnull\"></a>\r\n###### df.isnull\r\nUsed to detect missing values. As a reuslt, it returns booleans shaped as the object passed as parameter. \r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html)\r\n\r\n~~~python\r\n# detects missing values and returns as booleans\r\ndf.isnull()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/12.png)\r\n\r\n<a id=\"values\"></a>\r\n###### df.values\r\nReturn an array-like representation of the DataFrame. This property takes all the values of the object and returns each row as a list of values stored into a Numpy array.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html)\r\n\r\n~~~python\r\n# returns the df values as lists into a numpy array\r\ndf.values\r\n~~~\r\n\r\n![Image](../../../../images/post-2/13.png)\r\n\r\n#### Locating data\r\n\r\n<a id=\"loc\"></a>\r\n###### df.loc\r\nAccess a group of rows and colums by its labels. You can use this property to either access a unique item, an entire row, or any sort of slicing of rows throughout a column or a set of columns passed as input.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) for all the input possibilities.\r\n\r\n~~~python\r\n# access all rows stopping on label 3 of the columns Seller and Item\r\ndf.loc[:3, ['Seller', 'Item']]\r\n~~~\r\n\r\n![Image](../../../../images/post-2/14.png)\r\n\r\n<a id=\"iloc\"></a>\r\n###### df.iloc\r\nIt is an integer position based to locate and access itens of a DataFrame. It returns all the information about a specific row.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html)\r\n\r\n~~~python\r\n# selects the indexed row 2 of the DataFrame\r\ndf.iloc[2]\r\n~~~\r\n\r\n![Image](../../../../images/post-2/15.png)\r\n\r\n<a id=\"duplic\"></a>\r\n###### df.duplicated\r\nReturn a boolean Serie pointing the existence of duplicated rows. If no subset of columns is passed, this function will look for duplicated itens considering they are only equal if the entire rows match. You can also set the parameter `keep` as `{'first', 'last', False}` to indicate wheter you want to mark all duplicates except the *first* or *last* one as True, or if you'd like to mark all of them as True.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html)\r\n\r\n~~~python\r\n# check if there are duplicated rows when considering Seller and Item only\r\ndf.duplicated(['Seller', 'Item'])\r\n~~~\r\n\r\n![Image](../../../../images/post-2/16.png)\r\n\r\n<a id=\"query\"></a>\r\n###### df.query\r\nQuery the columns of a DataFrame and return where the passed expression is True.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)\r\n\r\n~~~python\r\n# queries all the values and returns where Seller is Carlos \r\ndf.query(\"Seller == 'Carlos'\")\r\n~~~\r\n\r\n![Image](../../../../images/post-2/17.png)\r\n\r\n<a id=\"df-col1\"></a>\r\n###### df['col']\r\nThis is on of the most common and simple ways to locate all the values on a entire column of a DataFrame. All you have to do is passing the columns names into the braces.\r\n\r\n~~~python\r\n# returns all the values into the column UnitPrice\r\ndf['UnitPrice']\r\n~~~\r\n\r\n![Image](../../../../images/post-2/18.png)\r\n\r\n<a id=\"df-col2\"></a>\r\n###### df.your_col\r\nFor a similar result as the previous locating method, you can also call the columns name as it is an attribute of the DataFrame.\r\n\r\n~~~python\r\n# returns all the values into the column UnitPrice\r\ndf.UnitPrice\r\n~~~\r\n\r\n![Image](../../../../images/post-2/18.png)\r\n\r\n#### Summarizing data\r\n\r\n<a id=\"describe\"></a>\r\n###### df.describe\r\nGenerate a decriptive statistics table. It includes measures such as mean, median, range, standard deviation, and more.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)\r\n\r\n~~~python\r\n# descriptive statistics with costumized percentiles\r\ndf.describe(percentiles=[0.2, 0.5, 0.8])\r\n~~~\r\n\r\n![Image](../../../../images/post-2/19.png)\r\n\r\n<a id=\"info\"></a>\r\n###### df.info\r\nPrint a summary of the DataFrame. This summary includes dtypes, columns, and non-null values.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)\r\n\r\n~~~python\r\n# prints summary info\r\ndf.info()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/20.png)\r\n\r\n<a id=\"sum\"></a>\r\n###### df.sum\r\nReturn the sum of the values. You can access a column first to have the results for that specific column.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html)\r\n\r\n~~~python\r\n# returns the sum for the column Units\r\ndf.Units.sum()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/21.png)\r\n\r\n<a id=\"count\"></a>\r\n###### df.count\r\nCount the cells with non-NA values for each row or column. NA values here are considered to be any *None*, *NaN*, *NaT* and *numpy.inf* values.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.count.html)\r\n\r\n~~~python\r\n# counts non-null cells for each column\r\ndf.count()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/22.png)\r\n\r\n<a id=\"min\"></a>\r\n###### df.min\r\nReturn the minimum of the values over the requested axis. You can either request for the minimum value of some row by passing `axis=0` or pass `axis=1` for the minimum over a column. \r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.min.html)\r\n\r\n~~~python\r\n# minimum values over the columns\r\ndf.min()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/23.png)\r\n\r\n<a id=\"max\"></a>\r\n###### df.max\r\nReturn the maximum of the values over the requested axis.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.max.html)\r\n\r\n~~~python\r\n# maximum value over the column UnitPrice\r\ndf.UnitPrice.max()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/24.png)\r\n\r\n<a id=\"mean\"></a>\r\n###### df.mean\r\nReturn the mean of the values over the requested axis.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html)\r\n\r\n~~~python\r\n# mean value over the column UnitPrice\r\ndf.UnitPrice.mean()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/25.png)\r\n\r\n<a id=\"median\"></a>\r\n###### df.median\r\nReturn the median of the values over the requested axis.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html)\r\n\r\n~~~python\r\n# median value over the column UnitPrice \r\ndf.UnitPrice.median()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/26.png)\r\n\r\n<a id=\"corr\"></a>\r\n###### df.corr\r\nCompute pairwise correlations of the DataFrame columns. It's very usefull to understand the correlation between two variables at a glance. A positive correlations indicates that the two variables varies on the same direction, while a negative correlation indicates that the two variables varies on opposite direction. On the other hand, a correlation of 0 indicates no real relationship between the variables.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)\r\n\r\n~~~python\r\n# chacking for existing correlation between numeric variables\r\ndf.corr()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/27.png)\r\n\r\n## Summary\r\nWe covered up many useful functions of the Pandas python package. These are just a lightly demonstration on how this package can be used when working with data. For the intent of this post, the chosen example were pretty simple; but when we are working with large sets of data, or creating machine learning models, they are undoubtedly life savers.\r\n\r\nDon't feel confortable with these simple examples, see the documentation to have a better understanding on everything you are able to do with them, and try if for youself on your own dataset. You'll see how easier it will become once you give it a try and compare the results by yourself!\r\n\r\n---\r\n###### Reference Links:\r\n\r\n[www.codingame.com](https://www.codingame.com/playgrounds/52723/programacao-python-parte-3---prof--marco-vaz/pacote-pandas-dataframe#:~:text=Uma%20S%C3%A9rie%20Pandas%3A%20um%20array,uma%20coluna%20de%20um%20DataFrame)<br/>\r\n[www.pandas.pydata.org](https://pandas.pydata.org/docs/index.html)"},{"__metadata":{"id":"content\\pages\\posts\\the-long-awaited-explanation-that-will-skill-you-on-telling-the-difference-among-ds-ai-ml-and-dl.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"posts\\the-long-awaited-explanation-that-will-skill-you-on-telling-the-difference-among-ds-ai-ml-and-dl.md","relProjectPath":"content\\pages\\posts\\the-long-awaited-explanation-that-will-skill-you-on-telling-the-difference-among-ds-ai-ml-and-dl.md","modelType":"page","modelName":"post","modelLabel":"Post","urlPath":"/posts/the-long-awaited-explanation-that-will-skill-you-on-telling-the-difference-among-ds-ai-ml-and-dl"},"frontmatter":{"title":"The long-awaited explanation that will skill you on telling the difference among DS, AI, ML and DL","subtitle":"Because we all have been hearing a lot about Data Science, Artificial Intelligence, Machine Learning, and Deep Learning lately.","excerpt":"It's not surprising that most of us have already heard something about artificial intelligence at least once in our life, right? Although the first concepts of artificial intelligence were mentioned way before its creation, it is not a so recent research. But many of us still get a little confused when other terminologies begun to appear. So if you have ever felt yourself stuck when trying to tell one from another, stick with me on this post and I'll make it clear as water for you.","date":"2021-04-13","thumb_img_path":"images/post-1/01.jpg","thumb_img_alt":"DS, AI, ML, DL","content_img_path":"images/post-1/01.jpg","seo":{"title":"The long-awaited explanation that will skill you on telling the difference among DS, AI, ML and DL","description":"Because we all have been hearing a lot about Data Science, Artificial Intelligence, Machine Learning, and Deep Learning lately.","extra":[{"name":"og:type","value":"article","keyName":"property"},{"name":"og:title","value":"The long-awaited explanation that will skill you on telling the difference among DS, AI, ML and DL","keyName":"property"},{"name":"og:description","value":"Because we all have been hearing a lot about Data Science, Artificial Intelligence, Machine Learning, and Deep Learning lately.","keyName":"property"},{"name":"og:image","value":"images/post-1/01.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"The long-awaited explanation that will skill you on telling the difference among DS, AI, ML and DL"},{"name":"twitter:description","value":"Because we all have been hearing a lot about Data Science, Artificial Intelligence, Machine Learning, and Deep Learning lately."},{"name":"twitter:image","value":"images/post-1/01.jpg","relativeUrl":true}]},"layout":"post"},"markdown":"\nRaise your hand who haven't ever thought \"What if...\" for the idea of having real intelligent machines among us. Or, still, who haven't ever seen a movie staring human-like robots?\n\nHumans have been fascinated for so long about the idea of having machines performing tasks only we could make, that the first time an Artificial Intelligence (AI) was put into the screen was in 1927. However, it was only during the World War II that we had real studies beginning on this particular field - and that is almost a hundred years from now!\n\nIt was basically because of this urging will to prove that we could create a computer able to think, communicate, and act like a human being, that we now have the ultimate advances we see on our daily life. Smart assistants (like Siri and Alexa), self-driving cars, translators, voice and facial recognizer, disease mapping, plan autopilot, credit card fraud prevention, personalized online marketing, and so many others are all everyday examples of AI been used. But these, what we have, these are way far from be at the finish line.\n\nAs technology earns its own place on the market, researchers raised many other terminologies to support the AI continuous evolution. Machine Learning (ML), Deep Learning (DL), and Data Science (DS) are some of the terms we noticed were added on the AI's toolbox. As they have been mentioned a lot in recent years, it's time for us to understand how they differ from each other, but also how they complete each other.\n\n### Artificial Intelligence\nEverything that make it possible for a machine to present signs of what we call human intelligence can be told as AI. That is, the concepts of AI were created to name all the effort we have been making to make sure that this obsession we have under the question \"Can machines think?\" becomes less and less an unreality.\n\nAI is nothing but our desperate need to prove ourselves that we can create a computer that is not only able to look like a human, but also to act, speak, learn, and think like we do. So it's not only having a cold computer program calculating formulas and giving you the results anymore, it is also having this computer learning from experiences and exhibiting signs of intelligence by itself.\n\nToday, this is still a very abstract concept, since the only type of AI we have is a goal oriented. It means that it is created to be smart at a very specific task, but it doesn't have a proper conscious of what it's been doing.\n\n> A computer would deserve to be called intelligent if it could deceive a human into believing that it was human. <cite>The Turing Test. Alan Turing, 1950</cite>\n\nHowever, studies have been also trying to achieve a more general type of AI, where machines will be able to have its own set of understanding, interpreting, and acting; becoming unpredictable, and indistinguishable from a human being in some given situation.\n\nAlthough this is still not part of our reality, many advances in technology have been substantial for people to start believing that we could, one day, achieve this sort of intelligence. But for now, having computers performing specific tasks and showing, sometimes, better results than a human being... This could be already called madness a few years ago.\n\n### Machine Learning\n![Image](../../../images/post-1/02.png)\n***image*** *by author*\n\nIt was when studies started to get really deeper into discovering how to make machines learn like humans, that the first concept of ML was introduced. In 1959, this field appears as an evolution of the computating learning theory and the pattern recognition studies.\n\n> Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed. <cite>Arthur Samuel, 1959</cite>\n\nScientists no longer wanted to only work on a trial and error system. They wanted to improve their ability to reach out a result that could be acceptably accurate. For that to happen, they saw the need to explore into the real meaning of learning; and so, later on, they come up with this concept that a human learning process could be put into some logic means through an algorithm, and explained to a machine. This machine would be then able to reproduce this \"logic\", they called the \"learning process\", and improve their results by retrieving errors as feedback. This so-fancy concept is nothing but what we already well-know as **learning through experience**.\n\nMany types of learning were developed on later years. But for now, we just need to understand that ML is a study that introduced the idea that we can make machines learn from experience - from data - without actually explicit programming their learning process and behavior.\n\nOnly by explaining the machines how to recognize patterns, researchers created a way to make computers develop a sufficient intelligence to take back their own errors and use them to improve their responses. This is Machine Learning. It is a subfield of AI, since it is the tool used to achieve many of the artificial intelligence applications we can see today. \n\n### Deep Learning\n![Deep Learning Interation](../../../images/post-1/03.png) \n***image*** *by author*\n\nTo be able to make the computer learning theory works with more complex kind of data, and to write the pipeline for the learning process as an algorithm that could interpret things like images or sounds, scientists did an awesome job recreating the human brain engine by simulating our neurons.\n\nI bet that sounds complicated, huh? Let me make it simpler.\n\nIn human beings, neurons are the basic structure responsible for carrying messages to and from our brain. Without them, we wouldn't be able to see, to hear, to feel, to taste, or to walk. It means that our brain needs to be entirely connected to our body in order for us to be able to do whatever thing we might want to do. And this \"connection\" is only possible because we have neurons - and many other structures - to carry electric impulses throughout our body. But machines don't have this connected body we do. So that's where the scientists came up with this insight.\n\n<cite>\"Maybe, we need to simulate a neuron!\"</cite>\n\nFrom that moment, when we found out that our brain worked just like our personal engine, researchers begin to create what they called the **artificial neuron**.\n\nNot all ML algorithms are powerful enough to work with *any* kind of information. Let's think of this in terms of our body. We may say that it's easier for us to understand that if we feel hungry we need to eat, than learning how to read when we still don't know what letters are for.\n\nSimilarly, some ML algorithms only work when the information we provide them are very organized and structured, just like our body is organized to know that eating is the natural action for when we feel hungry. This *little* issue brought up scientists the discussion about how to make machines understand data that are not so organized and structured, like images and audio files.\n\nThe artificial neurons were definitely a huge leap to the ML algorithms evolution. They are the concept of Deep Learning as it is.\n\nDL is a class of ML algorithms that use concepts way more complex, including multiple layers of data processing, to extract features from unorganized and non-structured data, that is, raw data. In other words, DL can be seen as an evolutionary complement of the ML algorithms, that has been used to bring us even closer to the initial idea of having general artificial intelligence created. Now, with the advances of DL algorithms, we can not only learn from basic set of data, but also break very complex data into small levels, and work with these smaller information into layers that will, at the end, bring all the processing together to come up with final features.\n\nBut you might be asking yourself <cite>\"Where does this Data Science thing go?\"</cite>\n\n### Data Science\nNow that we have all these technologies being created, it's time for us to think about its usability for good. And if you just though of DS as one of the ways to take advantage of these tools, you get it right, pal!\n\nData Science is an interdisciplinary field that involves a wide range of subjects, such as mathematics, statistics, computing processes, algorithms, business understanding, data analysis, and so on. The main goal of a *data scientist* is to extract valuable knowledge from information - called *data* -, in order to provide actionable insights in a broad range of application domain.\n\nTo be even clearer, DS is a field that can make use of the ML and DL concepts and tools to come up with insights that will turn into solutions for you.\n\nBut who might be *you*?\n\nYou could be anyone who need to solve a problem and who have a set of information, but still don't have a clue on how to use it. A data scientist will take her or his ability to work on big sets of data to manipulate, organize, and interpret them, and finally provide you with a clean image of what you need to turn your decision making process easier.\n\n### Summary\n![Image](../../../images/post-1/04.png)\n***image*** *by author*\n\nAnd to put all these information into one place, here is a final picture of the relationship among all these terminologies.\n\nBriefly, AI can be said as everything and every effort that make it possible for a machine to present signs of what we call human intelligence. Also, ML is a tool that we presented for the AI that allowed machines to learn by being exposed to external information. This way, we can understand that ML is part of the AI world.\n\nDigging a little bit deeper, we have DL that came as a complement to the ML algorithms to solve some restrictions when working with more complex type of data. That said, we can understand that DL is part of ML, being considered as a subclass within the whole package of ML algorithms.\n\nFinally, DS is well-known as a field of study that is able to take advantage of all of these tools, and combine them along with some mathematical, statistical, and analytical knowledge to extract valuable information for the most varied domains. \n\n---\n**Here you can have some more valuable information:** <br/>\n[Toward Data Science](https://towardsdatascience.com/understanding-the-difference-between-ai-ml-and-dl-cceb63252a6c)   (EN) <br/>\n[A História da Inteligência Artificial](https://www.youtube.com/watch?v=Lhu8bdmkMCM) (PT-BR)\n"},{"__metadata":{"id":"content\\pages\\success.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"success.md","relProjectPath":"content\\pages\\success.md","modelType":"page","modelName":"page","modelLabel":"Page","urlPath":"/success"},"frontmatter":{"title":"Thank You!","layout":"page"},"markdown":"\nThank you for contacting me! I will get back in touch with you soon.\n\n**Have a great day!**\n"},{"__metadata":{"id":"content\\pages\\todo-posts\\data-science-acronyms-cheatsheet.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"todo-posts\\data-science-acronyms-cheatsheet.md","relProjectPath":"content\\pages\\todo-posts\\data-science-acronyms-cheatsheet.md","modelType":"page","modelName":"page","modelLabel":"Page","urlPath":"/todo-posts/data-science-acronyms-cheatsheet"},"frontmatter":{"title":"Data Science acronyms cheatsheet.","date":"2021-04-05","tags":"java,spring boot,logging"},"markdown":""},{"__metadata":{"id":"content\\pages\\todo-posts\\http-status-codes.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"todo-posts\\http-status-codes.md","relProjectPath":"content\\pages\\todo-posts\\http-status-codes.md","modelType":"page","modelName":"page","modelLabel":"Page","urlPath":"/todo-posts/http-status-codes"},"frontmatter":{"title":"Main HTTP Status Codes for you to Favorite","date":"yyyy-mm-dd","tags":"java,spring boot,logging"},"markdown":""},{"__metadata":{"id":"content\\pages\\todo-posts\\what-is-an-API.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"todo-posts\\what-is-an-API.md","relProjectPath":"content\\pages\\todo-posts\\what-is-an-API.md","modelType":"page","modelName":"page","modelLabel":"Page","urlPath":"/todo-posts/what-is-an-api"},"frontmatter":{"title":"What in earth is an API?","date":"yyyy-mm-dd","tags":"java,spring boot,logging"},"markdown":""}],"data":{"__metadata":{"id":"sourcebit-source-filesystem:data","source":"sourcebit-source-filesystem"},"config":{"__metadata":{"id":"content\\data\\config.json","source":"sourcebit-source-filesystem","sourceName":"data","sourcePath":"content/data","relSourcePath":"config.json","relProjectPath":"content\\data\\config.json","modelType":"data","modelName":"config","modelLabel":"Site Configuration"},"title":"Stackbit Fjord Theme","path_prefix":"/","palette":"yellow","header":{"title":"Fjord","tagline":"Data Scientist","logo_img":"images/logo.svg","logo_img_alt":"Fjord Logo","background_img":"images/header-bg.JPG","has_nav":true,"nav_links":[{"label":"Home","url":"/","style":"link","__metadata":{"modelType":"object","modelName":"action","modelLabel":"Action"}},{"label":"About","url":"/about","style":"link","__metadata":{"modelType":"object","modelName":"action","modelLabel":"Action"}},{"label":"Portfolio","url":"/portfolio","style":"link","__metadata":{"modelType":"object","modelName":"action","modelLabel":"Action"}},{"label":"Contact","url":"/contact","style":"link","__metadata":{"modelType":"object","modelName":"action","modelLabel":"Action"}}],"has_social":true,"social_links":[{"label":"Twitter","url":"https://twitter.com/","style":"icon","icon_class":"twitter","new_window":true,"__metadata":{"modelType":"object","modelName":"action","modelLabel":"Action"}},{"label":"Instagram","url":"https://www.instagram.com/","style":"icon","icon_class":"instagram","new_window":true,"__metadata":{"modelType":"object","modelName":"action","modelLabel":"Action"}},{"label":"GitHub","url":"https://github.com/quelcamara","style":"icon","icon_class":"github","new_window":true,"__metadata":{"modelType":"object","modelName":"action","modelLabel":"Action"}},{"label":"LinkedIn","url":"https://www.linkedin.com/in/raquel-camara","style":"icon","icon_class":"linkedin","new_window":true,"__metadata":{"modelType":"object","modelName":"action","modelLabel":"Action"}},{"label":"DEV","url":"https://dev.to/","style":"icon","icon_class":"dev","new_window":true,"__metadata":{"modelType":"object","modelName":"action","modelLabel":"Action"}}],"__metadata":{"modelType":"object","modelName":"header","modelLabel":"Header Configuration"}},"footer":{"content":"&copy; Stackbit. All rights reserved. This Jamstack site was created with <a href=\"https://www.stackbit.com/?utm_source=deployed-footer\" target=\"_blank\" rel=\"noopener\">Stackbit</a>. Create yours <a href=\"https://app.stackbit.com/create?theme=fjord&utm_source=deployed-footer\" target=\"_blank\" rel=\"noopener\">now</a>","__metadata":{"modelType":"object","modelName":"footer","modelLabel":"Footer Configuration"}},"domain":"https://green-artichoke-e3ccb.netlify.app"}},"liveUpdate":true,"liveUpdatePort":8088,"liveUpdateEventName":"props_changed"},"pages":[{"path":"/","page":{"__metadata":{"id":"content\\pages\\index.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"index.md","relProjectPath":"content\\pages\\index.md","modelType":"page","modelName":"home","modelLabel":"Home","urlPath":"/"},"frontmatter":{"title":"Home","has_more_link":true,"more_link_text":"Keep reading","seo":{"title":"Raquel Câmara - Data Scientist","description":"A Data Scientist Diary","extra":[{"name":"og:type","value":"website","keyName":"property"},{"name":"og:title","value":"Stackbit Fjord Theme","keyName":"property"},{"name":"og:description","value":"A Data Scientist Diary","keyName":"property"},{"name":"og:image","value":"images/home.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"Stackbit Fjord Theme"},{"name":"twitter:description","value":"A Data Scientist Diary"},{"name":"twitter:image","value":"images/home.jpg","relativeUrl":true}]},"layout":"home"},"markdown":""}},{"path":"/contact","page":{"__metadata":{"id":"content\\pages\\contact.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"contact.md","relProjectPath":"content\\pages\\contact.md","modelType":"page","modelName":"contact","modelLabel":"Contact","urlPath":"/contact"},"frontmatter":{"title":"Get in Touch","img_path":"images/contact.jpg","form_id":"contactForm","form_action":"/success","form_fields":[{"input_type":"text","name":"name","label":"Name","default_value":"Your name","is_required":true},{"input_type":"email","name":"email","label":"Email","default_value":"Your email address","is_required":true},{"input_type":"select","name":"subject","label":"Subject","default_value":"Please select","options":["Get in touch","Post suggestions","Partnership","Error on the site","Other"]},{"input_type":"textarea","name":"message","label":"Message","default_value":"Your message"},{"input_type":"checkbox","name":"consent","label":"I understand that this form is storing my submitted information so I can be contacted."}],"submit_label":"Send Message","seo":{"title":"Get in Touch","description":"This is the contact page","extra":[{"name":"og:type","value":"website","keyName":"property"},{"name":"og:title","value":"Get in Touch","keyName":"property"},{"name":"og:description","value":"This is the contact page","keyName":"property"},{"name":"og:image","value":"images/contact.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"Get in Touch"},{"name":"twitter:description","value":"This is the contact page"},{"name":"twitter:image","value":"images/contact.jpg","relativeUrl":true}]},"layout":"contact"},"markdown":"\nTo get in touch fill the form below.\n"}},{"path":"/about","page":{"__metadata":{"id":"content\\pages\\about.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"about.md","relProjectPath":"content\\pages\\about.md","modelType":"page","modelName":"page","modelLabel":"Page","urlPath":"/about"},"frontmatter":{"title":"About Me","subtitle":"I am so glad to have you here!","img_path":"images/about.jpg","seo":{"title":"About Me","description":"A page about me and my work","extra":[{"name":"og:type","value":"website","keyName":"property"},{"name":"og:title","value":"About Me","keyName":"property"},{"name":"og:description","value":"A page about me and my work","keyName":"property"},{"name":"og:image","value":"images/about.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"About Me"},{"name":"twitter:description","value":"A page about me and my work"},{"name":"twitter:image","value":"images/about.jpg","relativeUrl":true}]},"layout":"page"},"markdown":"\nWelcome to the most beginner-friendly blog on **Data Science** you will ever see! Hope you are ready to go into the hotter topics on **Data Analysis**, **Machine Learning**, **Artificial Inteligence**, and so many other exciting stuff about technology and programming without getting all chaotic about those hard and complex book definitions.\n\nSo let us not take it too long!</br>\nBut first, let me introduce myself properly.\n\nI am a former Designer and Civil Engineer who ended up falling in love with Data Science after a long time struggling to decide if I should keep on following the path I had once begun, or if I should just throw it all away and follow my heart into something I could finally see myself in. I know it might sound a little cheesy at some point, but that's what it is.  \n\nI am a very curious person, and when I first dumped into Data Science, it was such a completely new world for me that I found myself unbearable amazed by the so many things I felt I just *needed* to know right away.\n\n>Any sufficiently advanced technology is indistinguishable from magic. <cite>Arthur C. Clarke</cite>\n\n So here we are!\n\nWhen I decided I wanted to keep track of my progress I found that building a blog would be just the perfect beginning. That's because here I can not only keep my records about the things I am most recently studying, but I can also improve some communication skills, and - last but not least! - maybe I am able to help other people, in the near future, who are also starting their jorney into Data Science.\n\nNo changing of career is easy as it seems. It demands a lot of effort and resilience from you to start all over again. And that's why I am here! I hope this blog finds you in a perfect timing so you don't give up on following the path you've been dreaming.\n\n*Be very welcome and enjoy your stay!* 💻\n"}},{"path":"/success","page":{"__metadata":{"id":"content\\pages\\success.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"success.md","relProjectPath":"content\\pages\\success.md","modelType":"page","modelName":"page","modelLabel":"Page","urlPath":"/success"},"frontmatter":{"title":"Thank You!","layout":"page"},"markdown":"\nThank you for contacting me! I will get back in touch with you soon.\n\n**Have a great day!**\n"}},{"path":"/todo-posts/data-science-acronyms-cheatsheet","page":{"__metadata":{"id":"content\\pages\\todo-posts\\data-science-acronyms-cheatsheet.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"todo-posts\\data-science-acronyms-cheatsheet.md","relProjectPath":"content\\pages\\todo-posts\\data-science-acronyms-cheatsheet.md","modelType":"page","modelName":"page","modelLabel":"Page","urlPath":"/todo-posts/data-science-acronyms-cheatsheet"},"frontmatter":{"title":"Data Science acronyms cheatsheet.","date":"2021-04-05","tags":"java,spring boot,logging"},"markdown":""}},{"path":"/todo-posts/http-status-codes","page":{"__metadata":{"id":"content\\pages\\todo-posts\\http-status-codes.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"todo-posts\\http-status-codes.md","relProjectPath":"content\\pages\\todo-posts\\http-status-codes.md","modelType":"page","modelName":"page","modelLabel":"Page","urlPath":"/todo-posts/http-status-codes"},"frontmatter":{"title":"Main HTTP Status Codes for you to Favorite","date":"yyyy-mm-dd","tags":"java,spring boot,logging"},"markdown":""}},{"path":"/todo-posts/what-is-an-api","page":{"__metadata":{"id":"content\\pages\\todo-posts\\what-is-an-API.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"todo-posts\\what-is-an-API.md","relProjectPath":"content\\pages\\todo-posts\\what-is-an-API.md","modelType":"page","modelName":"page","modelLabel":"Page","urlPath":"/todo-posts/what-is-an-api"},"frontmatter":{"title":"What in earth is an API?","date":"yyyy-mm-dd","tags":"java,spring boot,logging"},"markdown":""}},{"path":"/portfolio/insight-project-1","page":{"__metadata":{"id":"content\\pages\\portfolio\\insight-project-1.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"portfolio\\insight-project-1.md","relProjectPath":"content\\pages\\portfolio\\insight-project-1.md","modelType":"page","modelName":"post","modelLabel":"Post","urlPath":"/portfolio/insight-project-1"},"frontmatter":{"title":"Insight Project","subtitle":"Understanding the businesses problems is a key role to perform a good analysis. It's not unusual for the companies to not know exactly what their problems roots are. And that's when a data scientist will make a huge difference.","excerpt":"Explanatory Data Analysis to Build some Buy&Sell House Company Insights. The goal was to provide the company useful insights so they can better understand how the market and also their own businesses behave.","date":"2021-04-06","thumb_img_path":"images/portfolio-1/01.jpg","thumb_img_alt":"Greener Houses","content_img_path":"images/portfolio-1/01.jpg","seo":{"title":"Insight Project","description":"Understanding the businesses problems is a key role to perform a good analysis. It's not unusual for the companies to not know exactly what their problems roots are. And that's when a data scientist will make a huge difference.","extra":[{"name":"og:type","value":"article","keyName":"property"},{"name":"og:title","value":"Insight Project","keyName":"property"},{"name":"og:description","value":"Understanding the businesses problems is a key role to perform a good analysis. It's not unusual for the companies to not know exactly what their problems roots are. And that's when a data scientist will make a huge difference.","keyName":"property"},{"name":"og:image","value":"images/portfolio-1/01.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"Insight Project"},{"name":"twitter:description","value":"Understanding the businesses problems is a key role to perform a good analysis. It's not unusual for the companies to not know exactly what their problems roots are. And that's when a data scientist will make a huge difference."},{"name":"twitter:image","value":"images/portfolio-1/01.jpg","relativeUrl":true}]},"layout":"post"},"markdown":"\n<p align=\"left\"><a href=\"https://github.com/quelcamara/startup-success-analysis\"><img src=\"https://img.shields.io/badge/Language-Python-pink\"></a> <a href=\"https://github.com/quelcamara/startup-success-analysis\"><img src=\"https://img.shields.io/badge/Project%20Status-Finished-lightblue\"></a></p>\n\n## Table of Contents\n* [About the project](#about-the-project)\n* [Overall description](#overall-description)\n* [Becoming familiar](#becoming-familiar)\n  * [Business problem](#business-problem)\n  * [Solution planning](#solution-planning)\n  * [3 relevant insights](#3-relevant-insights)\n  * [Financial results](#financial-results)\n  * [Conclusion](#conclusion)\n* [Technologies](#technologies)\n* [Setups for Project Execution](#setups-for-project-execution)\n  * [Requirements](#requirements)\n  * [Installation](#installation)\n  * [Executing online](#executing-online)\n  * [Executing via cmd](#executing-via-cmd)\n* [Author](#author)\n\n### About the project\nThis is an insight project developed to be part of my data scientist portfolio. It is basically shaped as an explanatory data analysis to help a fictional company's decision makers to know whether to invest or not in a particular house.\n\nThe company, named Greener Houses, is in the market for 8 yrs with a business model focused on buying and selling houses through its digital platform. It is a very technological company and it performs all services based on market analysis decision making.\n\nThe main goal was to provide the company useful insights so they can better understand how the market behaves and also their own businesses. This way, they can see the hole picture for the businesses to come.\n\nAccess the complete project code [here](../../../images/portfolio-1/Greener_Houses.html) in html version. Alternatively, if you rather download the jupyter notebook, go to the GitHub repository [here](https://github.com/quelcamara/ds-portfolio-insight-project-01).\n\n### Overall description\nThe project was divided into 6 sections, each one of them also divided into subtopics. This repository includes the .csv file and a directory where the plots were saved. You will see under the sections:\n\n0. Business and Analysis Understanding:\n   - **Business Problem**: states the business questions to be answered along the analysis.\n   - **Business Assumptions**: stated the assumptions we make before starting manipulating the dataset. It includes the method we should use in case we need to remove outliers.\n   - **Dataset Description**: describes columns and data types for each attribute on the dataset.\n   - **Solution Planning**: describes the steps to be taking along the data analysis in order to achieve a good business problem resolution.  \n   - **Hypothesis**: states the initial assumptions we made before starting manipulating the dataset. These hypothesis guide us throughout the analysis.\n\n\n1. Imports:\n   - Loads packages, functions and the .csv file into the Jupyter Lab environment. \n\n\n2. Descriptive Analysis:\n   - It is the stage at which we become familiar with the data. This section includes measures of some statistical variables, such as *mean*, *median*, *mode*, *range*, and so on. It also includes data types, missing and duplicated values checking. Below is an example table for this section:\n\n![image](https://user-images.githubusercontent.com/73648823/114231149-6d975600-9950-11eb-98c8-7da8496c01f9.png)\n\n3. Data Cleaning and Preparation:\n   - Under this section, we discuss the need for preparing the dataset by cleaning unwanted data or not, depending on our goal.\n   - It also includes a *correlation matrix* plot to state how the attributes relate with one another.\n\n\n4. Exploratory Data Analysis:\n   - **Business Problem Nº1**: contains the resolution for the first business problem.\n   - **Business Problem Nº2**: contains the resolution for the second business problem.\n   \n\n5. Communicating Results:\n   - **Evaluating Hypothesis**: determines whether to support or reject the hypothesis.\n   - **Graphical Data Visualization**: illustrates some of the results graphically.\n  \n\n6. Conclusion\n   - Summarize the results by given the company a *prospective financial return* based on the analysis' suggestions. \n\n### Becoming familiar\n\n#### Business problem\nThe questions answered on this analysis are:\n- Which houses should Greener Houses buy and how much should the company pay for them?\n- Once the house is acquired, when should it be sold and at what price?\n- Should Greener Houses renovate a house to increase the sale price?\n\n#### Solution planning\nThe main steps taking to solve the business problem were as summarized below:\n- Collecting and organizing data;\n- Grouping data per region (zipcode);\n- Finding the houses prices median for each region;\n- Suggesting to buy the houses in good conditions that have prices lower than the median of its region.\n- Grouping data per seasonality;\n- Finding the new houses prices median within each set of region and seasonality;\n- Updating the suggestions based on the set of conditions analyzed.\n\n#### 3 relevant insights\nAfter carefully studying the dataset, some insights could be revealed. Three of them are listed below:\n\n1. When analyzing *if houses overlooking the waterfront were about 30% more expensive than houses not overlooking the waterfront on average*, we found out that, actually, their prices are more than **200%** higher comparing.\n\n![image](https://github.com/quelcamara/ds-portfolio-insight-project-01/blob/main/plots/average-prices-for-houses-either-overlooking-or-not-the-waterfront.png?raw=true)\n\n2. While expecting that houses with basements would have a total area (square foot lot) 40% larger than houses without basements on average, we found out that, on the opposite, houses without basements have a total area larger than houses with this spare room. This difference was calculated on 18% on average.\n\n![image](https://user-images.githubusercontent.com/73648823/114230229-40967380-994f-11eb-8796-67fae7e46832.png)\n\n3. If we first thought that houses with 3 bathrooms could have a MoM positive growth within time, on the analysis we actually discovered that they don't seem to have a meaningful growth and that their prices variation were very *miscellaneous*.\n\n![image](https://user-images.githubusercontent.com/73648823/114230160-28265900-994f-11eb-8433-68cfe5cc4a87.png)\n\n\n#### Financial results\nAs a result of this analysis, the company Greener Houses has about 11840 good deals on the construction market.\n\nIf the company chooses to invest only on the two more profitable regions out of the seventy available, it would still have 216 houses stated as good deals. This would represent a rate of 3 to 4 houses per month over the next 4 to 6 years, totalizing an average profit of U$234.344,38 per house and about U$700.000 to U$1.000.000 per month.\n\n#### Conclusion\nWith this analysis, Greener Houses is now capable of making its decisions based on the market opportunities, the financial returns, and also on the company revenue available to invest on these businesses. Moreover, the best time for selling and the profits for each deal will also be available for the company to evaluate prior to the decision-making.\n\nIt is then possible to say that the main goal of this analysis could be achieved.\n\n### Technologies\nThis project was built with the following technologies:\n* [Python](https://www.python.org/downloads/) --version: 3.9.2\n* [Jupyter Lab](https://jupyter.org/install) --version: 3.0.10\n* [Pandas](https://pandas.pydata.org/) --version: 1.2.3\n* [Numpy](https://numpy.org/) --version: 1.19.2\n* [Matplotlib](https://matplotlib.org/) --version: 3.2.1\n* [Seaborn](https://seaborn.pydata.org/) --version: 0.11.1\n\n### Setups for Project Execution\nIf you find it better to run the project on your computer rather than access the .html content, here are two options for you to open the .ipynb file: on your own localhost, or online.\n\nIf your only need is to see the notebook, there is no installation required, just jump into the [Executing online](#executing-online) topic and follow the directions.\n\nHowever, if you want to edit or manipulate the code, it will be necessary to have some basic configurations done on your machine to execute the project. See the on the topic [Requirements](#requirements) below.\n\n:bulb: You also won't be able to manipulate the file if you don't have your local server running the `Jupyter Lab`. For this, you can have a full explanation on the [Executing via cmd](#game_die-executing-via-cmd) step.\n\n#### Requirements\nBefore you get started, you will need to have [Python](https://www.python.org/downloads/) installed on your machine as well as the [Jupyter Notebook or Jupyter Lab](https://jupyter.org/install). There is no need to have any specific code editor or IDE if you wish to work on the code - you can do it directly on the notebooks.\n\nSince it is not externally deployed, you will need to download the [ds-portfolio-insight-project-01](https://github.com/quelcamara/ds-portfolio-insight-project-01) project into your computer to be able to run it locally.\n\n❗ But remember, if you only want to snoop into the code and have no interest on changing it, there is no need to trouble with this. You can also forget about the [Installation](#wrench-installation) and just go directly to the topic [Executing online](#globe_with_meridians-executing-online).\n\n##### Installation\nIf you have different versions of the technologies used to create this project, no need to panic! It won't be necessary to uninstall everything. You can always give it a try on running it on you machine with the current versions you already have and see if it works. If something goes wrong - because packages and functions are constantly being updated - you can easily create a virtual environment and then install only what you need.\n\nYou can follow the steps below if you need to create and configure a virtual environment. These are for Windows command-line, if you are a Linux or MacOS user, you will need to look for the likely commands. On the Command Prompt (cmd):\n\n```shell\n# Open the project directory (full path here)\n$ cd C:\\..\\portfolio-insight-project\n\n# Create the virtual environment (choosing python 3.9.2)\n$ virtualenv venv --python=python3.9.2\n\n# Access the virtual environment\n$ venv\\Scripts\\activate.bat\n```\nAfter that, you will see a `(venv)` sign on the command-line indicating that you are now inside the virtual environment. It should look like this:\n```shell\n$ (venv) C:\\..\\portfolio-insight-project>_\n```\nFinally, you can install the technologies you need with whatever version you want inside your environment, and it won't affect at all any package you have on your operational system. Use the `pip install` for that:\n\n> Just a friendly reminders here!<br/>\nIf you have installed <b>Anaconda</b> on your computer at any moment in your life, you probably already have all those packages available for you. Check for their versions and, if you have any troubles executing the Notebooks, create the virtual environment and install the packages using `conda` instead of `pip`.\n\n```shell\n$ pip install pandas==1.2.3\n$ pip install numpy==1.19.2\n$ pip install matplotlib==3.2.1\n$ pip install seaborn==0.11.1\n```\nWith these configurations, your machine is now ready to run and manipulate the project.\n\n##### Executing online\nTo run the notebook online, you'll first need to download the [Greener_Houses.ipynb](https://github.com/quelcamara/ds-portfolio-insight-project-01/blob/main/Greener_Houses.ipynb) file on this repository.\n\nThen, you can access a Jupyter Lab free and online server [here](https://jupyter.org/try). Scroll over the page and choose the JupyterLab option. It will redirect you to an online version of the tool that will allow you to open the file you have just downloaded.\n\nGo to your local repository where the downloaded file is and then drag the .ipynb file into the left sidebar of the online tool. The field should be seen with a hashed border before you drop it. Wait a little bit while it is loaded and then just double click on the project once it is there under the sidebar options.\n\n##### Executing via cmd\nTo run the project and have access to the Notebooks:\n```shell\n# Go to the project directory (full path here)\n$ cd C:\\..\\portfolio-insight-project\n\n# Open the directory on the Jupyter Lab\n$ jupyter lab\n```\nJust be patient here if your computer takes a little while without seeming to do a thing at all. It will process your inquisition and then open an external page on the web (your default browser) running the server on your localhost. You don't need to enter anything else on the command-line until this page is opened.\n"}},{"path":"/posts/3-mighty-libraries-for-a-data-science-beginner-to-become-friends-with-part-1","page":{"__metadata":{"id":"content\\pages\\posts\\3-mighty-libraries-for-a-data-science-beginner-to-become-friends-with-part-1.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"posts\\3-mighty-libraries-for-a-data-science-beginner-to-become-friends-with-part-1.md","relProjectPath":"content\\pages\\posts\\3-mighty-libraries-for-a-data-science-beginner-to-become-friends-with-part-1.md","modelType":"page","modelName":"post","modelLabel":"Post","urlPath":"/posts/3-mighty-libraries-for-a-data-science-beginner-to-become-friends-with-part-1"},"frontmatter":{"title":"3 mighty libraries for a Data Science beginner to become friends with (part 1)","subtitle":"I will tell you the secret for this friendship to work, and why you should stick with them forever.","excerpt":"It is very common to get confused about what we should learn at the very beginning of our studies. But no need to start yelling at a cat! I am sure that after these posts you will be gifted with plenty of working for the next few days. On this first section out of three posts, we'll discover the strength of Pandas.","date":"2021-05-13","thumb_img_path":"images/post-2/cover.jpg","thumb_img_alt":"migthy libraries","content_img_path":"images/post-2/cover.jpg","seo":{"title":"3 mighty libraries for a Data Science beginner to become friends with (part 1)","description":"I will tell you the secret for this friendship to work, and why you should stick with them forever.","extra":[{"name":"og:type","value":"article","keyName":"property"},{"name":"og:title","value":"3 mighty libraries for a Data Science beginner to become friends with (part 1)","keyName":"property"},{"name":"og:description","value":"I will tell you the secret for this friendship to work, and why you should stick with them forever.","keyName":"property"},{"name":"og:image","value":"images/post-2/cover.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"3 mighty libraries for a Data Science beginner to become friends with (part 1)"},{"name":"twitter:description","value":"I will tell you the secret for this friendship to work, and why you should stick with them forever."},{"name":"twitter:image","value":"images/post-2/cover.jpg","relativeUrl":true}]},"layout":"post"},"markdown":"\r\nI understand you.\r\n\r\nWhen we start something new, we use to get so excited that we begin to google search a lot of random stuff about that thing. And it becomes almost inevitable not to get lost on the endless sea of information available on the internet. But by putting some effort, we are able to filter what is important to know based on our current level.\r\n\r\nOn this set of posts, you will be given precious information about three **python** libraries that you should definitely look up:\r\n\r\n- Pandas\r\n- NumPy\r\n- Matplotlib\r\n\r\nYou probably already heard a thing or two about any of these. But if you didn't pay attention before - because maybe you thought they were not worth it - this is your chance to redeem yourself.\r\n\r\nTo better illustrate the next topics, consider the following dataset for demonstration purposes:\r\n\r\n&nbsp; | PurchaseDate |   Region  |   State   | Seller |  Item   | Units | UnitPrice  \r\n:-----:| :-----------:| :--------:| :--------:| :-----:| :------:| :----:| :-------:\r\n   0   | 10-Jun-2020  | Northeast |   Bahia   | Tobias | Stove   |  62   |  400.99\r\n   1   | 11-Jun-2020  | Southeast | São Paulo | Nadia  | Fridge  |  29   |  100.99\r\n   2   |  3-Aug-2020  | Northeast |   Ceará   | Carlos | Stove   |  55   | 1200.49\r\n   3   | 22-Aug-2020  | Northeast |   Bahia   | Pedro  | Fridge  |  81   | 1900.99\r\n   4   | 26-Aug-2020  |  Midwest  |   Goiás   | Tania  | Blender |  42   | 2300.95\r\n   5   | 10-Sep-2020  | Northeast |  Sergipe  | Tobias | Carpet  |  35   |  400.99\r\n   6   | 12-Sep-2020  |   North   |   Pará    | Carlos | Carpet  |   3   | 2750.00\r\n   7   |  7-Oct-2020  | Northeast |  Sergipe  | Nadia  | Blender |   2   | 1250.00\r\n   8   | 15-Oct-2020  |   North   | Amazonas  | Pedro  | Stove   |   7   | 1000.29\r\n   9   | 27-Nov-2020  | Southeast | São Paulo | Nadia  | Fridge  |  16   | 1500.99\r\n  10   | 13-Dec-2020  |   South   |  Paraná   | Tania  | Blender |  76   | 1450.99\r\n\r\n***file:*** order_data.csv\r\n\r\nOn this first post, we'll start with **Pandas**. And if you want to check on some specific functionality, I will leave you the table below so you can also go straight to the point.\r\n\r\n|    Load and Transform     |     Visualize     |           Locate           |       Summarize       |  \r\n:-------------------------: | :---------------: | :------------------------: | :-------------------: | \r\n[read_csv](#read-csv)       | [head](#head)     | [loc](#loc)                | [describe](#describe) |\r\n[read_excel](#read-excel)   | [tail](#tail)     | [iloc](#iloc)              | [info](#info)         |\r\n[sort_values](#sort-vals)   | [shape](#shape)   | [duplicated](#duplic)      | [sum](#sum)           |\r\n[set_index](#set-index)     | [index](#index)   | [query](#query)            | [count](#count)       |\r\n[reset_index](#reset-idx)   | [columns](#cols)  | [df['col']](#df-col1)      | [min](#min)           |\r\n[drop](#drop)               | [dtypes](#dtypes) | [df.your_col](#df-col2)    | [max](#max)           |\r\n[copy](#copy)               | [isnull](#isnull) |          ---               | [mean](#mean)         |\r\n---                         | [values](#values) |          ---               | [median](#median)     |\r\n---                         |        ---        |          ---               | [corr](#corr)         |\r\n\r\n\r\n## Pandas\r\n![Panda](../../images/post-2/01-panda.jpg)\r\n***image*** by [Eric Baccega](https://onebigphoto.com/giant-panda-sleeping-on-a-tree-china/)\r\n\r\nPandas is a very powerful python library widely used by data scientists and/or analysts for both manipulating and analysing data. It also works well with many other python modules, and its main advantage is having  an intuitive and practical usability without compromising its functionality.\r\n\r\nFor convenience, pandas use to be loaded into the project with the allias `pd`, as shown below:\r\n\r\n~~~python\r\nimport pandas as pd\r\n\r\n~~~\r\n\r\nIt is useful so we can just use this short allias instead of typing the whole package name every time we want to use a pandas function. This library also allows us to create two type of structures that make the manipulating easier: **Series** and **Dataframes**.\r\n\r\n#### Series\r\nAccording to pandas official [documentation](https://pandas.pydata.org/docs/index.html), a series is a one-dimensional ndarray (an array that belongs to the NumPy class `ndarray`) with axis labels.\r\n\r\n*Omg, this...*\r\n\r\nBetter saying, a pandas series is nothing but an unidimensional array (having a unique dimension) which can store any sort of data with labels or indexes as the axis. In short, it is like a column of a dataframe. Let's see how to create a series in pandas and how to identify its structure. The following code can be replicated in your jupyter notebook:\r\n\r\n~~~python\r\n# imports\r\nIn [1]: import pandas as pd\r\n\r\n# creates a series to store people names\r\nIn [2]: names = pd.Series(['Carlos', 'Sara', 'Louise', 'James'])\r\n\r\nIn [3]: names\r\nOut[3]: 0   Carlos\r\n        1     Sara\r\n        2   Louise\r\n        3    James\r\n        dtype: object\r\n\r\n~~~\r\n\r\nNote that, as indexation in python begins with 0, the index range of the series created goes from 0 to `n-1`, where *n* is the number of elements on your series. This is a very important thing to remember, and which might cause a lot of malfunctioning on your codes if you mess that up.\r\n\r\nWe can also see that, as I haven't specified any index range for my series, pandas automaticatly insert the standard indexation. However, if I want a different label for my axis, I can specify that when creating the object. See how it looks:\r\n\r\n~~~python\r\n# creates a series to store animals species\r\nIn [2]: animals = pd.Series(['Dog', 'Elephant', 'Fox', 'Eagle'],\r\n                            index=['A', 'B', 'C', 'D'])\r\n\r\nIn [3]: animals\r\nOut[3]: A        Dog\r\n        B   Elephant\r\n        C        Fox\r\n        D      Eagle\r\n        dtype: object\r\n\r\n~~~\r\n\r\nThese are, in fact, very simple examples. But they might help you having an ideia of what a series look like.\r\n\r\n#### Dataframes\r\nDifferently from a series, a pandas dataframe is a two-dimensional tabular structure where data is labeled by its own combination of column and row. This structure is size-mutable and potentially heterogeneous. That is, we can easily create a dataframe with two columns and two rows, and then add new columns and rows for this same object. And we can store different types of data on the same dataframe, which can be very convinient in many situations.\r\n\r\nLet's see how it works on the jupyter notebook:\r\n\r\n~~~python\r\n# imports\r\nIn [1]: import pandas as pd\r\n\r\n# creates a dataframe to store people names, ages, and heights\r\nIn [2]: names = pd.DataFrame([['Carlos', 27, 1.78], ['Sara', 12, 1.35],\r\n                              ['Louise', 35, 1.62], ['James', 18, 1.87]],\r\n                              columns=['name', 'age', 'height'],\r\n                              index=['i', 'ii','iii', 'iv'])\r\n\r\nIn [3]: names\r\nOut[3]:      name   age   height\r\n        i  Carlos    27     1.78\r\n       ii    Sara    12     1.35\r\n      iii  Louise    35     1.62\r\n       iv   James    18     1.87\r\n~~~\r\n\r\nCould you notice how they differ? Now we have a table storing a set of information that we can either access by the index - returning all the information in a the row - or by the combination of index and columns - returning a single desired value.\r\n\r\nAnd if we just want to go on regular indexing, we only need to remove the `index` attribute from the function.\r\n\r\nSee the official [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) for more information.\r\n\r\nJust so to remind you, all the operations from now on will take into consideration the dataset we defiined on the very beginning of this post. Also, see that you will face many abbreviations and acronyms on your way through the data science world. I will provide you a cheatsheet on later posts. \r\n\r\nNow, let us finally see what pandas can bring us!\r\n\r\n#### Loading data\r\nPandas has many functions to load data into your project. You can mine data from a csv - a text file - or an excel spreadsheet, for instance. But it is also possible to get information from SQL and HTML tables, SQL query, JSON strings, Google BigQuery, Stata .dta files, and so on. See [here](https://pandas.pydata.org/docs/reference/io.html) all the options pandas offers.\r\n\r\nHere are two frequently used functions:\r\n\r\n<a id=\"read-csv\"></a>\r\n###### pd.read_csv\r\nFor comma-separated text files.\r\n\r\n~~~python\r\n# locate and indicate your file path\r\ndf = pd.read_csv('C:\\...\\order_data.csv')\r\n~~~\r\n\r\n<a id=\"read-excel\"></a>\r\n###### pd.read_excel\r\nFor excel files.\r\n\r\n~~~python\r\n# locate and indicate your file path\r\n# you can also indicate the sheet name if there are more than one\r\ndf = pd.read_csv('C:\\...\\your_file_here.xlsx',\r\n                      sheet_name='sheet_name_here')\r\n~~~\r\n\r\n#### Transforming data\r\nSometimes we only need to perform some basic transformations on our DataFrame and that's where this functions come in handy.\r\n\r\n<a id=\"sort-vals\"></a>\r\n###### df.sort_values\r\nSort DataFrame by the values of chosen column or columns. It is possible to sort twice if you pass more than one column as parameter. You can also choose the direction of your sorting by assigning `ascending` as `True` or `False`.\r\n\r\nSee the [documentation](#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html).\r\n\r\n~~~python\r\n# sorts in ascending order by the column 'Seller'\r\ndf.sort_values(by='Seller', ascending=True)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/01.png)\r\n\r\n<a id=\"set-index\"></a>\r\n###### df.set_index\r\nSet an existing DataFrame column as the index. You can either use it to replace the original index or to expand it.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html).\r\n\r\n~~~python\r\n# using append=True to expand the index\r\n# column 'Seller' is picked\r\ndf.set_index('Seller', append=True)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/02.png)\r\n\r\n<a id=\"reset-idx\"></a>\r\n###### df.reset_index\r\nReset the index of a DataFrame, using the default one instead. Default indexation in python begin with 0. You can either drop the current index or insert it as a column into the DataFrame. \r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html)\r\n\r\n~~~python\r\n# drop=False keeps the current index as a column\r\ndf.reset_index(drop=False)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/03.png)\r\n\r\n<a id=\"drop\"></a>\r\n###### df.drop\r\nRemove rows or columns by specifying row index or column name to drop.<br/>\r\nFor `axis=0`, the function searchs through the DataFrame indexes. <br/>\r\nFor `axis=1`, it searchs through its columns.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html)\r\n\r\n~~~python\r\n# removes PurchaseDate from the columns\r\ndf.drop('PurchaseDate', axis=1)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/04.png)\r\n\r\n<a id=\"copy\"></a>\r\n###### df.copy\r\nMake a copy of the object's indices and data. If you set `deep=True`, none of the modifications on the original object will reflect on the copy. However, setting `deep=False` makes a shadow copy, and any modification on either the original or the shadow object will reflect on each other.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html)\r\n\r\n~~~python\r\n# creates a copy df\r\n# deep=True is the default parameter\r\ndf_2 = df.copy()\r\nprint(df_2)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/05.png)\r\n\r\n#### Visualizing data\r\n\r\n<a id=\"head\"></a>\r\n###### df.head\r\nReturn the **first** *n* rows. If `n` is not specified, it returns the first 5 rows as default. It is aso possible to return everything except the last *n* rows by passing a negative parameter for `n`.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)\r\n\r\n~~~python\r\n# returns the first 3 rows\r\ndf.head(3)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/06.png)\r\n\r\n<a id=\"tail\"></a>\r\n###### df.tail\r\nReturn the **last** *n* rows. If `n` is not specified, it returns the last 5 rows as default. It is aso possible to return everything except the first *n* rows by passing a negative parameter for `n`.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tail.html)\r\n\r\n~~~python\r\n# returns everything except the first 6 rows\r\ndf.tail(-6)\r\n~~~\r\n\r\n![Image](../../../../images/post-2/07.png)\r\n\r\n<a id=\"shape\"></a>\r\n###### df.shape\r\nReturn a tuple representing the dimensionality of the DataFrame. The first element represents the total number of rows, and the second element is the total number of columns.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html)\r\n\r\n~~~python\r\n# returns the DataFrame dimensionality\r\ndf.shape\r\n~~~\r\n\r\n![Image](../../../../images/post-2/08.png)\r\n\r\n<a id=\"index\"></a>\r\n###### df.index\r\nReturn the index (row labels) of the DataFrame. If the object index is the default indexation, it will return a RangeIndex object with `start`, `stop`, and `step` parameters.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.index.html)\r\n\r\n~~~python\r\n# returns rows labels\r\ndf.index\r\n~~~\r\n\r\n![Image](../../../../images/post-2/09.png)\r\n\r\n<a id=\"cols\"></a>\r\n###### df.columns\r\nReturn a list of the columns labels of the DataFrame.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html)\r\n\r\n~~~python\r\n# returns columns labels\r\ndf.columns\r\n~~~\r\n\r\n![Image](../../../../images/post-2/10.png)\r\n\r\n<a id=\"dtypes\"></a>\r\n###### df.dtypes\r\nIt returns a Series with the data type of each column. If there are columns with mixed types, they'll be stored with the *object* `dtype`.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html)\r\n\r\n~~~python\r\n# returns the data type of each column\r\ndf.dtypes\r\n~~~\r\n\r\n![Image](../../../../images/post-2/11.png)\r\n\r\n<a id=\"isnull\"></a>\r\n###### df.isnull\r\nUsed to detect missing values. As a reuslt, it returns booleans shaped as the object passed as parameter. \r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html)\r\n\r\n~~~python\r\n# detects missing values and returns as booleans\r\ndf.isnull()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/12.png)\r\n\r\n<a id=\"values\"></a>\r\n###### df.values\r\nReturn an array-like representation of the DataFrame. This property takes all the values of the object and returns each row as a list of values stored into a Numpy array.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html)\r\n\r\n~~~python\r\n# returns the df values as lists into a numpy array\r\ndf.values\r\n~~~\r\n\r\n![Image](../../../../images/post-2/13.png)\r\n\r\n#### Locating data\r\n\r\n<a id=\"loc\"></a>\r\n###### df.loc\r\nAccess a group of rows and colums by its labels. You can use this property to either access a unique item, an entire row, or any sort of slicing of rows throughout a column or a set of columns passed as input.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) for all the input possibilities.\r\n\r\n~~~python\r\n# access all rows stopping on label 3 of the columns Seller and Item\r\ndf.loc[:3, ['Seller', 'Item']]\r\n~~~\r\n\r\n![Image](../../../../images/post-2/14.png)\r\n\r\n<a id=\"iloc\"></a>\r\n###### df.iloc\r\nIt is an integer position based to locate and access itens of a DataFrame. It returns all the information about a specific row.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html)\r\n\r\n~~~python\r\n# selects the indexed row 2 of the DataFrame\r\ndf.iloc[2]\r\n~~~\r\n\r\n![Image](../../../../images/post-2/15.png)\r\n\r\n<a id=\"duplic\"></a>\r\n###### df.duplicated\r\nReturn a boolean Serie pointing the existence of duplicated rows. If no subset of columns is passed, this function will look for duplicated itens considering they are only equal if the entire rows match. You can also set the parameter `keep` as `{'first', 'last', False}` to indicate wheter you want to mark all duplicates except the *first* or *last* one as True, or if you'd like to mark all of them as True.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html)\r\n\r\n~~~python\r\n# check if there are duplicated rows when considering Seller and Item only\r\ndf.duplicated(['Seller', 'Item'])\r\n~~~\r\n\r\n![Image](../../../../images/post-2/16.png)\r\n\r\n<a id=\"query\"></a>\r\n###### df.query\r\nQuery the columns of a DataFrame and return where the passed expression is True.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)\r\n\r\n~~~python\r\n# queries all the values and returns where Seller is Carlos \r\ndf.query(\"Seller == 'Carlos'\")\r\n~~~\r\n\r\n![Image](../../../../images/post-2/17.png)\r\n\r\n<a id=\"df-col1\"></a>\r\n###### df['col']\r\nThis is on of the most common and simple ways to locate all the values on a entire column of a DataFrame. All you have to do is passing the columns names into the braces.\r\n\r\n~~~python\r\n# returns all the values into the column UnitPrice\r\ndf['UnitPrice']\r\n~~~\r\n\r\n![Image](../../../../images/post-2/18.png)\r\n\r\n<a id=\"df-col2\"></a>\r\n###### df.your_col\r\nFor a similar result as the previous locating method, you can also call the columns name as it is an attribute of the DataFrame.\r\n\r\n~~~python\r\n# returns all the values into the column UnitPrice\r\ndf.UnitPrice\r\n~~~\r\n\r\n![Image](../../../../images/post-2/18.png)\r\n\r\n#### Summarizing data\r\n\r\n<a id=\"describe\"></a>\r\n###### df.describe\r\nGenerate a decriptive statistics table. It includes measures such as mean, median, range, standard deviation, and more.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)\r\n\r\n~~~python\r\n# descriptive statistics with costumized percentiles\r\ndf.describe(percentiles=[0.2, 0.5, 0.8])\r\n~~~\r\n\r\n![Image](../../../../images/post-2/19.png)\r\n\r\n<a id=\"info\"></a>\r\n###### df.info\r\nPrint a summary of the DataFrame. This summary includes dtypes, columns, and non-null values.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)\r\n\r\n~~~python\r\n# prints summary info\r\ndf.info()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/20.png)\r\n\r\n<a id=\"sum\"></a>\r\n###### df.sum\r\nReturn the sum of the values. You can access a column first to have the results for that specific column.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html)\r\n\r\n~~~python\r\n# returns the sum for the column Units\r\ndf.Units.sum()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/21.png)\r\n\r\n<a id=\"count\"></a>\r\n###### df.count\r\nCount the cells with non-NA values for each row or column. NA values here are considered to be any *None*, *NaN*, *NaT* and *numpy.inf* values.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.count.html)\r\n\r\n~~~python\r\n# counts non-null cells for each column\r\ndf.count()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/22.png)\r\n\r\n<a id=\"min\"></a>\r\n###### df.min\r\nReturn the minimum of the values over the requested axis. You can either request for the minimum value of some row by passing `axis=0` or pass `axis=1` for the minimum over a column. \r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.min.html)\r\n\r\n~~~python\r\n# minimum values over the columns\r\ndf.min()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/23.png)\r\n\r\n<a id=\"max\"></a>\r\n###### df.max\r\nReturn the maximum of the values over the requested axis.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.max.html)\r\n\r\n~~~python\r\n# maximum value over the column UnitPrice\r\ndf.UnitPrice.max()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/24.png)\r\n\r\n<a id=\"mean\"></a>\r\n###### df.mean\r\nReturn the mean of the values over the requested axis.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html)\r\n\r\n~~~python\r\n# mean value over the column UnitPrice\r\ndf.UnitPrice.mean()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/25.png)\r\n\r\n<a id=\"median\"></a>\r\n###### df.median\r\nReturn the median of the values over the requested axis.\r\n\r\nSee the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html)\r\n\r\n~~~python\r\n# median value over the column UnitPrice \r\ndf.UnitPrice.median()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/26.png)\r\n\r\n<a id=\"corr\"></a>\r\n###### df.corr\r\nCompute pairwise correlations of the DataFrame columns. It's very usefull to understand the correlation between two variables at a glance. A positive correlations indicates that the two variables varies on the same direction, while a negative correlation indicates that the two variables varies on opposite direction. On the other hand, a correlation of 0 indicates no real relationship between the variables.\r\n\r\nSee the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)\r\n\r\n~~~python\r\n# chacking for existing correlation between numeric variables\r\ndf.corr()\r\n~~~\r\n\r\n![Image](../../../../images/post-2/27.png)\r\n\r\n## Summary\r\nWe covered up many useful functions of the Pandas python package. These are just a lightly demonstration on how this package can be used when working with data. For the intent of this post, the chosen example were pretty simple; but when we are working with large sets of data, or creating machine learning models, they are undoubtedly life savers.\r\n\r\nDon't feel confortable with these simple examples, see the documentation to have a better understanding on everything you are able to do with them, and try if for youself on your own dataset. You'll see how easier it will become once you give it a try and compare the results by yourself!\r\n\r\n---\r\n###### Reference Links:\r\n\r\n[www.codingame.com](https://www.codingame.com/playgrounds/52723/programacao-python-parte-3---prof--marco-vaz/pacote-pandas-dataframe#:~:text=Uma%20S%C3%A9rie%20Pandas%3A%20um%20array,uma%20coluna%20de%20um%20DataFrame)<br/>\r\n[www.pandas.pydata.org](https://pandas.pydata.org/docs/index.html)"}},{"path":"/posts/the-long-awaited-explanation-that-will-skill-you-on-telling-the-difference-among-ds-ai-ml-and-dl","page":{"__metadata":{"id":"content\\pages\\posts\\the-long-awaited-explanation-that-will-skill-you-on-telling-the-difference-among-ds-ai-ml-and-dl.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"posts\\the-long-awaited-explanation-that-will-skill-you-on-telling-the-difference-among-ds-ai-ml-and-dl.md","relProjectPath":"content\\pages\\posts\\the-long-awaited-explanation-that-will-skill-you-on-telling-the-difference-among-ds-ai-ml-and-dl.md","modelType":"page","modelName":"post","modelLabel":"Post","urlPath":"/posts/the-long-awaited-explanation-that-will-skill-you-on-telling-the-difference-among-ds-ai-ml-and-dl"},"frontmatter":{"title":"The long-awaited explanation that will skill you on telling the difference among DS, AI, ML and DL","subtitle":"Because we all have been hearing a lot about Data Science, Artificial Intelligence, Machine Learning, and Deep Learning lately.","excerpt":"It's not surprising that most of us have already heard something about artificial intelligence at least once in our life, right? Although the first concepts of artificial intelligence were mentioned way before its creation, it is not a so recent research. But many of us still get a little confused when other terminologies begun to appear. So if you have ever felt yourself stuck when trying to tell one from another, stick with me on this post and I'll make it clear as water for you.","date":"2021-04-13","thumb_img_path":"images/post-1/01.jpg","thumb_img_alt":"DS, AI, ML, DL","content_img_path":"images/post-1/01.jpg","seo":{"title":"The long-awaited explanation that will skill you on telling the difference among DS, AI, ML and DL","description":"Because we all have been hearing a lot about Data Science, Artificial Intelligence, Machine Learning, and Deep Learning lately.","extra":[{"name":"og:type","value":"article","keyName":"property"},{"name":"og:title","value":"The long-awaited explanation that will skill you on telling the difference among DS, AI, ML and DL","keyName":"property"},{"name":"og:description","value":"Because we all have been hearing a lot about Data Science, Artificial Intelligence, Machine Learning, and Deep Learning lately.","keyName":"property"},{"name":"og:image","value":"images/post-1/01.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"The long-awaited explanation that will skill you on telling the difference among DS, AI, ML and DL"},{"name":"twitter:description","value":"Because we all have been hearing a lot about Data Science, Artificial Intelligence, Machine Learning, and Deep Learning lately."},{"name":"twitter:image","value":"images/post-1/01.jpg","relativeUrl":true}]},"layout":"post"},"markdown":"\nRaise your hand who haven't ever thought \"What if...\" for the idea of having real intelligent machines among us. Or, still, who haven't ever seen a movie staring human-like robots?\n\nHumans have been fascinated for so long about the idea of having machines performing tasks only we could make, that the first time an Artificial Intelligence (AI) was put into the screen was in 1927. However, it was only during the World War II that we had real studies beginning on this particular field - and that is almost a hundred years from now!\n\nIt was basically because of this urging will to prove that we could create a computer able to think, communicate, and act like a human being, that we now have the ultimate advances we see on our daily life. Smart assistants (like Siri and Alexa), self-driving cars, translators, voice and facial recognizer, disease mapping, plan autopilot, credit card fraud prevention, personalized online marketing, and so many others are all everyday examples of AI been used. But these, what we have, these are way far from be at the finish line.\n\nAs technology earns its own place on the market, researchers raised many other terminologies to support the AI continuous evolution. Machine Learning (ML), Deep Learning (DL), and Data Science (DS) are some of the terms we noticed were added on the AI's toolbox. As they have been mentioned a lot in recent years, it's time for us to understand how they differ from each other, but also how they complete each other.\n\n### Artificial Intelligence\nEverything that make it possible for a machine to present signs of what we call human intelligence can be told as AI. That is, the concepts of AI were created to name all the effort we have been making to make sure that this obsession we have under the question \"Can machines think?\" becomes less and less an unreality.\n\nAI is nothing but our desperate need to prove ourselves that we can create a computer that is not only able to look like a human, but also to act, speak, learn, and think like we do. So it's not only having a cold computer program calculating formulas and giving you the results anymore, it is also having this computer learning from experiences and exhibiting signs of intelligence by itself.\n\nToday, this is still a very abstract concept, since the only type of AI we have is a goal oriented. It means that it is created to be smart at a very specific task, but it doesn't have a proper conscious of what it's been doing.\n\n> A computer would deserve to be called intelligent if it could deceive a human into believing that it was human. <cite>The Turing Test. Alan Turing, 1950</cite>\n\nHowever, studies have been also trying to achieve a more general type of AI, where machines will be able to have its own set of understanding, interpreting, and acting; becoming unpredictable, and indistinguishable from a human being in some given situation.\n\nAlthough this is still not part of our reality, many advances in technology have been substantial for people to start believing that we could, one day, achieve this sort of intelligence. But for now, having computers performing specific tasks and showing, sometimes, better results than a human being... This could be already called madness a few years ago.\n\n### Machine Learning\n![Image](../../../images/post-1/02.png)\n***image*** *by author*\n\nIt was when studies started to get really deeper into discovering how to make machines learn like humans, that the first concept of ML was introduced. In 1959, this field appears as an evolution of the computating learning theory and the pattern recognition studies.\n\n> Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed. <cite>Arthur Samuel, 1959</cite>\n\nScientists no longer wanted to only work on a trial and error system. They wanted to improve their ability to reach out a result that could be acceptably accurate. For that to happen, they saw the need to explore into the real meaning of learning; and so, later on, they come up with this concept that a human learning process could be put into some logic means through an algorithm, and explained to a machine. This machine would be then able to reproduce this \"logic\", they called the \"learning process\", and improve their results by retrieving errors as feedback. This so-fancy concept is nothing but what we already well-know as **learning through experience**.\n\nMany types of learning were developed on later years. But for now, we just need to understand that ML is a study that introduced the idea that we can make machines learn from experience - from data - without actually explicit programming their learning process and behavior.\n\nOnly by explaining the machines how to recognize patterns, researchers created a way to make computers develop a sufficient intelligence to take back their own errors and use them to improve their responses. This is Machine Learning. It is a subfield of AI, since it is the tool used to achieve many of the artificial intelligence applications we can see today. \n\n### Deep Learning\n![Deep Learning Interation](../../../images/post-1/03.png) \n***image*** *by author*\n\nTo be able to make the computer learning theory works with more complex kind of data, and to write the pipeline for the learning process as an algorithm that could interpret things like images or sounds, scientists did an awesome job recreating the human brain engine by simulating our neurons.\n\nI bet that sounds complicated, huh? Let me make it simpler.\n\nIn human beings, neurons are the basic structure responsible for carrying messages to and from our brain. Without them, we wouldn't be able to see, to hear, to feel, to taste, or to walk. It means that our brain needs to be entirely connected to our body in order for us to be able to do whatever thing we might want to do. And this \"connection\" is only possible because we have neurons - and many other structures - to carry electric impulses throughout our body. But machines don't have this connected body we do. So that's where the scientists came up with this insight.\n\n<cite>\"Maybe, we need to simulate a neuron!\"</cite>\n\nFrom that moment, when we found out that our brain worked just like our personal engine, researchers begin to create what they called the **artificial neuron**.\n\nNot all ML algorithms are powerful enough to work with *any* kind of information. Let's think of this in terms of our body. We may say that it's easier for us to understand that if we feel hungry we need to eat, than learning how to read when we still don't know what letters are for.\n\nSimilarly, some ML algorithms only work when the information we provide them are very organized and structured, just like our body is organized to know that eating is the natural action for when we feel hungry. This *little* issue brought up scientists the discussion about how to make machines understand data that are not so organized and structured, like images and audio files.\n\nThe artificial neurons were definitely a huge leap to the ML algorithms evolution. They are the concept of Deep Learning as it is.\n\nDL is a class of ML algorithms that use concepts way more complex, including multiple layers of data processing, to extract features from unorganized and non-structured data, that is, raw data. In other words, DL can be seen as an evolutionary complement of the ML algorithms, that has been used to bring us even closer to the initial idea of having general artificial intelligence created. Now, with the advances of DL algorithms, we can not only learn from basic set of data, but also break very complex data into small levels, and work with these smaller information into layers that will, at the end, bring all the processing together to come up with final features.\n\nBut you might be asking yourself <cite>\"Where does this Data Science thing go?\"</cite>\n\n### Data Science\nNow that we have all these technologies being created, it's time for us to think about its usability for good. And if you just though of DS as one of the ways to take advantage of these tools, you get it right, pal!\n\nData Science is an interdisciplinary field that involves a wide range of subjects, such as mathematics, statistics, computing processes, algorithms, business understanding, data analysis, and so on. The main goal of a *data scientist* is to extract valuable knowledge from information - called *data* -, in order to provide actionable insights in a broad range of application domain.\n\nTo be even clearer, DS is a field that can make use of the ML and DL concepts and tools to come up with insights that will turn into solutions for you.\n\nBut who might be *you*?\n\nYou could be anyone who need to solve a problem and who have a set of information, but still don't have a clue on how to use it. A data scientist will take her or his ability to work on big sets of data to manipulate, organize, and interpret them, and finally provide you with a clean image of what you need to turn your decision making process easier.\n\n### Summary\n![Image](../../../images/post-1/04.png)\n***image*** *by author*\n\nAnd to put all these information into one place, here is a final picture of the relationship among all these terminologies.\n\nBriefly, AI can be said as everything and every effort that make it possible for a machine to present signs of what we call human intelligence. Also, ML is a tool that we presented for the AI that allowed machines to learn by being exposed to external information. This way, we can understand that ML is part of the AI world.\n\nDigging a little bit deeper, we have DL that came as a complement to the ML algorithms to solve some restrictions when working with more complex type of data. That said, we can understand that DL is part of ML, being considered as a subclass within the whole package of ML algorithms.\n\nFinally, DS is well-known as a field of study that is able to take advantage of all of these tools, and combine them along with some mathematical, statistical, and analytical knowledge to extract valuable information for the most varied domains. \n\n---\n**Here you can have some more valuable information:** <br/>\n[Toward Data Science](https://towardsdatascience.com/understanding-the-difference-between-ai-ml-and-dl-cceb63252a6c)   (EN) <br/>\n[A História da Inteligência Artificial](https://www.youtube.com/watch?v=Lhu8bdmkMCM) (PT-BR)\n"}},{"path":"/portfolio","page":{"__metadata":{"id":"content\\pages\\portfolio.md","source":"sourcebit-source-filesystem","sourceName":"pages","sourcePath":"content/pages","relSourcePath":"portfolio.md","relProjectPath":"content\\pages\\portfolio.md","modelType":"page","modelName":"portfolio","modelLabel":"Portfolio","urlPath":"/portfolio"},"frontmatter":{"title":"Portfolio","subtitle":"Topic under construction.","img_path":"images/portfolio.jpg","seo":{"title":"Portfolio","description":"My data science portfolio","extra":[{"name":"og:type","value":"website","keyName":"property"},{"name":"og:title","value":"Portfolio","keyName":"property"},{"name":"og:description","value":"My data science portfolio","keyName":"property"},{"name":"og:image","value":"images/portfolio.jpg","keyName":"property","relativeUrl":true},{"name":"twitter:card","value":"summary_large_image"},{"name":"twitter:title","value":"Portfolio"},{"name":"twitter:description","value":"My data science portfolio"},{"name":"twitter:image","value":"images/portfolio.jpg","relativeUrl":true}]},"layout":"portfolio"},"markdown":""}}]}
